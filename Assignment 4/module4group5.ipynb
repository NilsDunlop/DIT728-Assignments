{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0e7365d49bc45f1",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Group 5 - Module 4: Diagnostic Systems\n",
    "\n",
    "***\n",
    "### Group Members:\n",
    "* **Nils Dunlop, 20010127-2359, Applied Data Science, e-mail: gusdunlni@student.gu.se (16 hours)**\n",
    "* **Francisco Erazo, 19930613-9214, Applied Data Science, e-mail: guserafr@student.gu.se (16 hours)**\n",
    "\n",
    "#### **We hereby declare that we have both actively participated in solving every exercise. All solutions are entirely our own work, without having taken part of other solutions.\" (This is independent and additional to any declaration that you may encounter in the electronic submission system.)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a40ce4f1465244c7",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Assignment 4\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "426429e48a4209ce",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Problem 1: Reading and Reflection\n",
    "***\n",
    "### Machine Learning Techniques to Diagnose Breast Cancer from Image-Processed Nuclear Features of Fine Needle Aspirates\n",
    "- **Summary**:   \n",
    "The paper introduces an interactive computer system that evaluates and diagnoses breast cancer using cytologic features derived from digital scans of fine-needle aspirates (FNAs) slides. This study involved a dataset of 569 patients (212 of which were diagnosed with breast cancer) to develop the system and tested it on a separate dataset of 54 patients (36 benign, 1 atypia and 17 malignant cases). It achieved a 97% accuracy throught 10-fold cross-validation and 100% accuracy on the test set. The system utilizes digital image analysis and machine learning techniques to enhance diagnostic accuracy for breast FNAs, indicating significant potential for early detection and treatment of breast cancer.\n",
    "\n",
    "- **Key Takeaways**:\n",
    "  - **Data**: They aspirated a small drop of viscous fluid from the breast massess and smeared it on a glass slide, and scanned it with a high-resolution digital camera. The digital images were then analyzed to extract 30 features for each case (radius, perimeter, area, smoothness, compactness, concavity, concave points, symmetry, fractal dimension and texture).\n",
    "\n",
    "  - **Digital Image Analysis**: One of the main benefits of this techonology is that provides measurements of shape features besides size and texture features provided by scanning techniques. \n",
    "\n",
    "  - **MSM-Tree**: They implement MSM-Tree (MSM-T) to classify the observatinos between bening and malignant sets. Then this classifier was used to categorize new observations. They highlight that simpler classifiers outperformed more complex ones on new data. The best results were obtained with a single-plane classifier based on three nuclear features mean texture, the worst area and the worst smoothness. \n",
    "\n",
    "  - **Clinical Applications**: The system estimates the probability of an instance being malignant, this information shared with the patient so they can make a decision about performing a biopsy or monitor an apparently benign mass.\n",
    "\n",
    "  - **High Accuracy**: The system achieved a 97% predictive accuracy through 10-fold cross-validation estimating that the true prospective accuracy range between 95.5% and 98.5%. In the test set, the accuracy was 100%, indicating that the system is highly effective in diagnosing breast cancer. Important to remark they only used cell features. \n",
    "  \n",
    "  - **Advancement in Breast Cancer Detection**: The study represents an important advancement in the use of technology for breast cancer detection, indicating the potential of integrating AI and Machine Learning into the healthcare field. They also highlight that computer-assisted image analysis was better than human interpretation of the images.\n",
    "\n",
    "### The Mythos of Model Interpretability\n",
    "- **Summary**:     \n",
    "The article discusses the complex nature of interpretability within machine learning models. It highlights the lack of a unified definition of interpretability encountered in the academic literature, the diverse motivations behind seeking interpretable models, and the distinction between model transparency (understanding how a model works) and post-hoc explanations (understanding hmodel prediction after they are made).\n",
    "\n",
    "- **Key Takeaways**:    \n",
    "  - **Need for Interpretability**: Interpretability is needed when there’s a gap between what supervised learning aims to do and the real-world consequences. Sometimes, just having predictions and metrics isn’t enough to understand the model.\n",
    "    - **Trust**: Being able to interpret a model might be necessary for trust. Trust could mean believing that the model will do well in real situations. It could also mean being okay with letting the model take control.\n",
    "    \n",
    "    - **Causality**: Supervised learning models are often used hoping to learn about the natural world. But, the connections these models learn don’t always show cause and effect relationships.\n",
    "    \n",
    "    - **Transferability**: Humans are good at applying what they’ve learned to new situations. Machine learning models are also used in these situations, like when the environment changes or might be against them.\n",
    "    \n",
    "    - **Informativeness**: Sometimes, supervised models are used to give information to people making decisions. An interpretation can be useful even if it doesn’t explain how the model works.\n",
    "    \n",
    "    - **Fairness and Ethics**: Interpretations are needed to check if decisions made by algorithms are ethical. Usual evaluation metrics don’t guarantee that decisions made by machine learning will be acceptable, leading to demands for fairness and interpretable models.\n",
    "\n",
    "  - **Transparency**: This refers to understanding how the model works. It can be viewed at different levels:\n",
    "\n",
    "    - **Simulatability**: A model is considered transparent if a person can understand the entire model at once. This suggests that simpler models are more interpretable.\n",
    "\n",
    "    - **Decomposability**: This means that each part of the model (input, parameter, calculation) can be explained intuitively.\n",
    "\n",
    "    - **Algorithmic Transparency**: This applies to the learning algorithm itself. For example, in linear models, you can understand the shape of the error surface and prove that training will converge to a unique solution.\n",
    "\n",
    "  - **Post hoc interpretability**: This approach extracts information from learned models after they have been trained. It doesn’t necessarily explain how a model works, but it can provide useful information. Some common approaches include:\n",
    "    \n",
    "    - **Text Explanations**: One model generates predictions, and a separate model generates an explanation.\n",
    "    \n",
    "    - **Visualization**: Visualizations are rendered to determine what a model has learned.\n",
    "    \n",
    "    - **Local Explanations**: These explain what a model depends on locally. For example, a saliency map highlights regions of the input that would most influence the output if changed.\n",
    "\n",
    "    - **Explanation by Example**: This involves reporting which other examples are most similar with respect to the model.\n",
    "\n",
    "  - **Linear vs Deep Models**: Linear models are not always more interpretable than deep neural networks. The level of interpretability depends on the specific definition used. For example, deep neural networks can have an advantage in post hoc interpretation because they learn rich representations that can be visualized or used for clustering.\n",
    "\n",
    "  - **Claims about Interpretability**: Any claim about a model’s interpretability should be specific and meaningful. It should clearly define what form of interpretability is being referred to, such as transparency or post hoc interpretability. Also, the desire for transparency should not compromise the broader objectives of AI, such as improving healthcare.\n",
    "\n",
    "  - **Potential Misleading of Post hoc Interpretations**: Post hoc interpretations can potentially mislead, especially when they are optimized to meet subjective demands. They might present plausible but misleading explanations. Therefore, it’s important to be cautious and avoid reproducing harmful behavior when using machine learning models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1b6dbabbbd02376",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Problem 2: Implementation\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d263170",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "abc48290c4db11b3",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Problem 3: Discussion\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1a71987",
   "metadata": {},
   "source": [
    "## References: \n",
    "***\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c902e53e01be6b5",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Self Check\n",
    "***\n",
    "- Have you answered all questions to the best of your ability?\n",
    "Yes, we have.\n",
    "- Is all the required information on the front page, is the file name correct etc.?\n",
    "Indeed, all the required information on the front page has been included.\n",
    "- Anything else you can easily check? (details, terminology, arguments, clearly stated answers etc.?)\n",
    "We have checked, and everything looks good."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
