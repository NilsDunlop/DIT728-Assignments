{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0e7365d49bc45f1",
   "metadata": {
    "id": "b0e7365d49bc45f1"
   },
   "source": [
    "# Group 5 - Module 5: Natural Language Processing\n",
    "\n",
    "***\n",
    "### Group Members:\n",
    "* **Nils Dunlop, 20010127-2359, Applied Data Science, e-mail: gusdunlni@student.gu.se (16 hours)**\n",
    "* **Francisco Erazo, 19930613-9214, Applied Data Science, e-mail: guserafr@student.gu.se (16 hours)**\n",
    "\n",
    "#### **We hereby declare that we have both actively participated in solving every exercise. All solutions are entirely our own work, without having taken part of other solutions.\" (This is independent and additional to any declaration that you may encounter in the electronic submission system.)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a40ce4f1465244c7",
   "metadata": {
    "id": "a40ce4f1465244c7"
   },
   "source": [
    "# Assignment 5\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "426429e48a4209ce",
   "metadata": {
    "id": "426429e48a4209ce"
   },
   "source": [
    "## Problem 1: Reading and Reflection\n",
    "***\n",
    "### **(A) AI Problems with a Wide Range of Approaches:**\n",
    "\n",
    "Several AI problems have seen a similarly wide array of approaches, reflecting the many computational techniques utilized over the decades. Some notable examples include:\n",
    "- **Speech Recognition:** From early phonetic-based systems to modern deep learning approaches, speech recognition has evolved significantly. Early systems relied on simple pattern matching and rule-based systems to understand spoken language. Over time, statistical models like Hidden Markov Models (HMMs) played a crucial role, and now deep neural networks, particularly recurrent neural networks (RNNs) and convolutional neural networks (CNNs) dominate the field.\n",
    "   \n",
    "- **Computer Vision:** The evolution of computer vision techniques from simple edge detection algorithms and feature extraction methods to sophisticated deep learning models is another example. Techniques have ranged from geometric model fitting and template matching to the use of deep convolutional neural networks (CNNs) for tasks such as image classification, object detection, and semantic segmentation.\n",
    "   \n",
    "- **Game Playing:** The development of AI for game playing, from chess and checkers to complex video games, has seen strategies evolve from brute-force search algorithms and heuristic-based approaches to the use of machine learning techniques, including reinforcement learning and deep learning for strategy optimization.\n",
    "\n",
    "### **(B) Similarities Between Rule-based Translation Systems and Neural Systems:**\n",
    "\n",
    "Despite their differences, there are notable similarities between some aspects of rule-based translation systems and state-of-the-art neural systems:\n",
    "\n",
    "- **Structured Knowledge:** Rule-based systems are explicitly programmed with linguistic rules and dictionaries. Neural systems, particularly those employing attention mechanisms, implicitly learn to encode structured knowledge about languages through training on large datasets, creating internal representations that mirror some aspects of linguistic rules.\n",
    "\n",
    "- **Context Handling:** Both systems attempt to handle context in translation, albeit in radically different ways. Rule-based systems may use context rules to choose the correct translation of a word based on surrounding words. Neural systems, especially those using mechanisms like attention, are able to consider broader context in a more fluid and dynamic manner to understand the appropriate meaning.\n",
    "   \n",
    "- **Error Correction:** Both systems have mechanisms for error correction, although neural systems do this implicitly. Rule-based systems may use post-processing rules to correct common mistakes, while neural systems learn error patterns and their corrections during training.\n",
    "\n",
    "### **(C) Situations Favoring Rule-based Solutions Over Modern Approaches:**\n",
    "\n",
    "There are scenarios where a rule-based solution might be preferable to a modern neural or statistical approach:\n",
    "\n",
    "- **Limited Data Scenarios:** For languages or specialized domains where there is a scarcity of training data, rule-based systems can be more practical and effective because they do not require large datasets to perform reasonably well.\n",
    "   \n",
    "- **Predictability and Transparency:** In situations where predictability, transparency, and the ability to audit or explain decisions are crucial (e.g., in some legal or regulatory contexts), rule-based systems offer clear advantages. Their decisions are based on explicit rules that can be examined, understood, and modified by humans.\n",
    "\n",
    "- **Real-time Constraints:** In cases where computational resources are limited or real-time performance is essential, the relative simplicity and lower computational requirements of rule-based systems can be an advantage over more resource-intensive neural models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1b6dbabbbd02376",
   "metadata": {
    "id": "b1b6dbabbbd02376"
   },
   "source": [
    "## Problem 2: Implementation\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### (A) Warmup: Word Frequencies\n",
    "***"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c2344500c97686b5"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              German  \\\n",
      "0  ich erkläre die am freitag , dem 17. dezember ...   \n",
      "1  wie sie feststellen konnten , ist der gefürcht...   \n",
      "2  im parlament besteht der wunsch nach einer aus...   \n",
      "3  heute möchte ich sie bitten - das ist auch der...   \n",
      "4  wie sie sicher aus der presse und dem fernsehe...   \n",
      "\n",
      "                                          English_DE  \n",
      "0  i declare resumed the session of the european ...  \n",
      "1  although , as you will have seen , the dreaded...  \n",
      "2  you have requested a debate on this subject in...  \n",
      "3  in the meantime , i should like to observe a m...  \n",
      "4  you will be aware from the press and televisio...  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import re\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import time\n",
    "\n",
    "# Load datasets into DataFrames\n",
    "de_en_df = pd.DataFrame({\n",
    "    'German': pd.read_csv(\"dat410_europarl/europarl-v7.de-en.lc.de\", sep='\\n', header=None, names=['text'], squeeze=True),\n",
    "    'English_DE': pd.read_csv(\"dat410_europarl/europarl-v7.de-en.lc.en\", sep='\\n', header=None, names=['text'], squeeze=True)\n",
    "})\n",
    "\n",
    "fr_en_df = pd.DataFrame({\n",
    "    'French': pd.read_csv(\"dat410_europarl/europarl-v7.fr-en.lc.fr\", sep='\\n', header=None, names=['text'], squeeze=True),\n",
    "    'English_FR': pd.read_csv(\"dat410_europarl/europarl-v7.fr-en.lc.en\", sep='\\n', header=None, names=['text'], squeeze=True)\n",
    "})\n",
    "\n",
    "sv_en_df = pd.DataFrame({\n",
    "    'Swedish': pd.read_csv(\"dat410_europarl/europarl-v7.sv-en.lc.sv\", sep='\\n', header=None, names=['text'], squeeze=True),\n",
    "    'English_SV': pd.read_csv(\"dat410_europarl/europarl-v7.sv-en.lc.en\", sep='\\n', header=None, names=['text'], squeeze=True)\n",
    "})\n",
    "\n",
    "# Display the first few rows of the German-English dataset\n",
    "print(de_en_df.head())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-20T12:03:20.955677100Z",
     "start_time": "2024-02-20T12:03:16.577814600Z"
    }
   },
   "id": "817613a5b9b4d9cb",
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              French  \\\n",
      "0  je déclare reprise la session du parlement eur...   \n",
      "1  comme vous avez pu le constater , le grand &qu...   \n",
      "2  vous avez souhaité un débat à ce sujet dans le...   \n",
      "3  en attendant , je souhaiterais , comme un cert...   \n",
      "4  je vous invite à vous lever pour cette minute ...   \n",
      "\n",
      "                                          English_FR  \n",
      "0  i declare resumed the session of the european ...  \n",
      "1  although , as you will have seen , the dreaded...  \n",
      "2  you have requested a debate on this subject in...  \n",
      "3  in the meantime , i should like to observe a m...  \n",
      "4  please rise , then , for this minute &apos; s ...  \n"
     ]
    }
   ],
   "source": [
    "# Display the first few rows of the French-English dataset\n",
    "print(fr_en_df.head())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-20T12:03:20.974666300Z",
     "start_time": "2024-02-20T12:03:20.906664800Z"
    }
   },
   "id": "843e900b35ca21fe",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             Swedish  \\\n",
      "0  jag förklarar europaparlamentets session återu...   \n",
      "1  som ni kunnat konstatera ägde &quot; den stora...   \n",
      "2  ni har begärt en debatt i ämnet under sammantr...   \n",
      "3  till dess vill jag att vi , som ett antal koll...   \n",
      "4             jag ber er resa er för en tyst minut .   \n",
      "\n",
      "                                          English_SV  \n",
      "0  i declare resumed the session of the european ...  \n",
      "1  although , as you will have seen , the dreaded...  \n",
      "2  you have requested a debate on this subject in...  \n",
      "3  in the meantime , i should like to observe a m...  \n",
      "4  please rise , then , for this minute &apos; s ...  \n"
     ]
    }
   ],
   "source": [
    "# Display the first few rows of the Swedish-English dataset\n",
    "print(sv_en_df.head())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-20T12:03:21.121661100Z",
     "start_time": "2024-02-20T12:03:20.983663500Z"
    }
   },
   "id": "4d02f8867e40f6f4",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Function to calculate word frequencies\n",
    "def calculate_word_frequencies(df_column):\n",
    "    word_counts = Counter()\n",
    "    for text in df_column:\n",
    "        # Remove punctuation and other non-word characters\n",
    "        words = re.findall(r'\\b\\w+\\b', text.lower())\n",
    "        word_counts.update(words)\n",
    "    return word_counts"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-20T12:03:21.153668500Z",
     "start_time": "2024-02-20T12:03:21.107656300Z"
    }
   },
   "id": "3c9bd61df230e8d2",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Calculate word frequencies for each language\n",
    "de_word_counts = calculate_word_frequencies(de_en_df['German'])\n",
    "en_de_word_counts = calculate_word_frequencies(de_en_df['English_DE'])\n",
    "\n",
    "fr_word_counts = calculate_word_frequencies(fr_en_df['French'])\n",
    "en_fr_word_counts = calculate_word_frequencies(fr_en_df['English_FR'])\n",
    "\n",
    "sv_word_counts = calculate_word_frequencies(sv_en_df['Swedish'])\n",
    "en_sv_word_counts = calculate_word_frequencies(sv_en_df['English_SV'])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-20T12:03:22.188657600Z",
     "start_time": "2024-02-20T12:03:21.174659800Z"
    }
   },
   "id": "adcf00330048fa75",
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Function to print most common words\n",
    "def print_most_common(word_counts, language, num=10):\n",
    "    print(f\"Most common words in {language}:\")\n",
    "    for word, count in word_counts.most_common(num):\n",
    "        print(f\"{word}: {count}\")\n",
    "    print(\"\\n\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-20T12:03:22.190662Z",
     "start_time": "2024-02-20T12:03:22.178656600Z"
    }
   },
   "id": "c66efd3074499963",
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most common words in German:\n",
      "die: 10521\n",
      "der: 9374\n",
      "und: 7028\n",
      "in: 4175\n",
      "zu: 3169\n",
      "den: 2976\n",
      "wir: 2863\n",
      "daß: 2738\n",
      "ich: 2670\n",
      "das: 2669\n",
      "\n",
      "\n",
      "Most common words in English (DE):\n",
      "the: 19853\n",
      "of: 9633\n",
      "to: 9069\n",
      "and: 7307\n",
      "in: 6278\n",
      "is: 4478\n",
      "that: 4441\n",
      "a: 4438\n",
      "we: 3372\n",
      "this: 3362\n",
      "\n",
      "\n",
      "Most common words in French:\n",
      "apos: 16729\n",
      "de: 14528\n",
      "la: 9746\n",
      "et: 6620\n",
      "l: 6536\n",
      "le: 6177\n",
      "à: 5588\n",
      "les: 5587\n",
      "des: 5232\n",
      "que: 4797\n",
      "\n",
      "\n",
      "Most common words in English (FR):\n",
      "the: 19627\n",
      "of: 9534\n",
      "to: 8992\n",
      "and: 7214\n",
      "in: 6197\n",
      "is: 4453\n",
      "that: 4421\n",
      "a: 4388\n",
      "we: 3341\n",
      "this: 3332\n",
      "\n",
      "\n",
      "Most common words in Swedish:\n",
      "att: 9181\n",
      "och: 7038\n",
      "i: 5954\n",
      "det: 5687\n",
      "som: 5028\n",
      "för: 4959\n",
      "av: 4013\n",
      "är: 3840\n",
      "en: 3724\n",
      "vi: 3211\n",
      "\n",
      "\n",
      "Most common words in English (SV):\n",
      "the: 19327\n",
      "of: 9344\n",
      "to: 8814\n",
      "and: 6949\n",
      "in: 6124\n",
      "is: 4400\n",
      "that: 4357\n",
      "a: 4271\n",
      "we: 3223\n",
      "this: 3222\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print the 10 most frequent words for each language\n",
    "print_most_common(de_word_counts, \"German\")\n",
    "print_most_common(en_de_word_counts, \"English (DE)\")\n",
    "\n",
    "print_most_common(fr_word_counts, \"French\")\n",
    "print_most_common(en_fr_word_counts, \"English (FR)\")\n",
    "\n",
    "print_most_common(sv_word_counts, \"Swedish\")\n",
    "print_most_common(en_sv_word_counts, \"English (SV)\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-20T12:03:22.230660500Z",
     "start_time": "2024-02-20T12:03:22.189653900Z"
    }
   },
   "id": "1c678acf838a13c",
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability of 'speaker' in German: 0.000000\n",
      "Probability of 'zebra' in German: 0.000000\n",
      "Probability of 'speaker' in English (DE): 0.000042\n",
      "Probability of 'zebra' in English (DE): 0.000000\n",
      "Probability of 'speaker' in French: 0.000000\n",
      "Probability of 'zebra' in French: 0.000000\n",
      "Probability of 'speaker' in English (FR): 0.000046\n",
      "Probability of 'zebra' in English (FR): 0.000000\n",
      "Probability of 'speaker' in Swedish: 0.000000\n",
      "Probability of 'zebra' in Swedish: 0.000000\n",
      "Probability of 'speaker' in English (SV): 0.000039\n",
      "Probability of 'zebra' in English (SV): 0.000000\n"
     ]
    }
   ],
   "source": [
    "# Calculate and print probabilities for 'speaker' and 'zebra' in each language\n",
    "def print_probabilities(word_counts, language, total_words):\n",
    "    for word in ['speaker', 'zebra']:\n",
    "        prob = word_counts[word] / total_words if word in word_counts else 0\n",
    "        print(f\"Probability of '{word}' in {language}: {prob:.6f}\")\n",
    "\n",
    "total_de_words = sum(de_word_counts.values())\n",
    "total_en_de_words = sum(en_de_word_counts.values())\n",
    "total_fr_words = sum(fr_word_counts.values())\n",
    "total_en_fr_words = sum(en_fr_word_counts.values())\n",
    "total_sv_words = sum(sv_word_counts.values())\n",
    "total_en_sv_words = sum(en_sv_word_counts.values())\n",
    "\n",
    "print_probabilities(de_word_counts, \"German\", total_de_words)\n",
    "print_probabilities(en_de_word_counts, \"English (DE)\", total_en_de_words)\n",
    "print_probabilities(fr_word_counts, \"French\", total_fr_words)\n",
    "print_probabilities(en_fr_word_counts, \"English (FR)\", total_en_fr_words)\n",
    "print_probabilities(sv_word_counts, \"Swedish\", total_sv_words)\n",
    "print_probabilities(en_sv_word_counts, \"English (SV)\", total_en_sv_words)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-20T12:03:22.255652500Z",
     "start_time": "2024-02-20T12:03:22.227664300Z"
    }
   },
   "id": "2616bdd5e8d0c0ba",
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "source": [
    "### (B) Language Modeling\n",
    "***"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "603802a163b48ed7"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Tokenize the text and add start and end tokens\n",
    "tokenized_text_de = [('<START>',) + tuple(re.findall(r'\\b\\w+\\b', sentence.lower())) + ('<END>',) for sentence in de_en_df['English_DE']]\n",
    "tokenized_text_fr = [('<START>',) + tuple(re.findall(r'\\b\\w+\\b', sentence.lower())) + ('<END>',) for sentence in fr_en_df['English_FR']]\n",
    "tokenized_text_sv = [('<START>',) + tuple(re.findall(r'\\b\\w+\\b', sentence.lower())) + ('<END>',) for sentence in sv_en_df['English_SV']]\n",
    "\n",
    "# Count individual words and bigrams\n",
    "word_counts_de = Counter()\n",
    "bigram_counts_de = Counter()\n",
    "word_counts_fr = Counter()\n",
    "bigram_counts_fr = Counter()\n",
    "word_counts_sv = Counter()\n",
    "bigram_counts_sv = Counter()\n",
    "\n",
    "for sentence in tokenized_text_de:\n",
    "    word_counts_de.update(sentence)\n",
    "    bigram_counts_de.update(zip(sentence[:-1], sentence[1:]))\n",
    "\n",
    "for sentence in tokenized_text_fr:\n",
    "    word_counts_fr.update(sentence)\n",
    "    bigram_counts_fr.update(zip(sentence[:-1], sentence[1:]))\n",
    "\n",
    "for sentence in tokenized_text_sv:\n",
    "    word_counts_sv.update(sentence)\n",
    "    bigram_counts_sv.update(zip(sentence[:-1], sentence[1:]))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-20T12:03:23.261657700Z",
     "start_time": "2024-02-20T12:03:22.252666600Z"
    }
   },
   "id": "b3af9522c34a79e7",
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability of the sentence 'The economic impact of the legislation was significant.' in German-English data: 5.452363580933155e-28\n",
      "Probability of the sentence 'Diplomatic efforts were intensified to resolve the conflict.' in French-English data: 5.274458597849841e-34\n",
      "Probability of the sentence 'Environmental policies have become increasingly important.' in Swedish-English data: 1.381206394015978e-26\n",
      "Probability of the sentence with an OOV word 'this is a quixotic test sentence' in German-English data: 1.0607721443308697e-21\n",
      "Probability of the sentence with an OOV word 'this is a quixotic test sentence' in French-English data: 1.0477026424612885e-21\n",
      "Probability of the sentence with an OOV word 'this is a quixotic test sentence' in Swedish-English data: 1.0820221311118906e-21\n"
     ]
    }
   ],
   "source": [
    "# Compute the probability of a sentence using a bigram model with Laplace smoothing\n",
    "def bigram_sentence_probability(sentence, word_counts, bigram_counts, total_words, smoothing=1):\n",
    "    sentence = ('<START>',) + tuple(re.findall(r'\\b\\w+\\b', sentence.lower())) + ('<END>',)\n",
    "    bigram_probs = []\n",
    "\n",
    "    for first_word, second_word in zip(sentence[:-1], sentence[1:]):\n",
    "        bigram_count = bigram_counts[(first_word, second_word)]\n",
    "        word_count = word_counts[first_word]\n",
    "        # Apply Laplace smoothing\n",
    "        prob = (bigram_count + smoothing) / (word_count + smoothing * (total_words + 1))\n",
    "        bigram_probs.append(prob)\n",
    "\n",
    "    # Use logarithms to avoid underflow with long sentences\n",
    "    log_probs = np.log(bigram_probs)\n",
    "    log_prob_sentence = np.sum(log_probs)\n",
    "\n",
    "    return np.exp(log_prob_sentence)\n",
    "\n",
    "# Test the bigram_sentence_probability for the German-English dataset\n",
    "de_sentence = \"The economic impact of the legislation was significant.\"\n",
    "de_prob = bigram_sentence_probability(de_sentence, word_counts_de, bigram_counts_de, len(word_counts_de))\n",
    "print(f\"Probability of the sentence '{de_sentence}' in German-English data: {de_prob}\")\n",
    "\n",
    "# Test the bigram_sentence_probability for the French-English dataset\n",
    "fr_sentence = \"Diplomatic efforts were intensified to resolve the conflict.\"\n",
    "fr_prob = bigram_sentence_probability(fr_sentence, word_counts_fr, bigram_counts_fr, len(word_counts_fr))\n",
    "print(f\"Probability of the sentence '{fr_sentence}' in French-English data: {fr_prob}\")\n",
    "\n",
    "# Test the bigram_sentence_probability for the Swedish-English dataset\n",
    "sv_sentence = \"Environmental policies have become increasingly important.\"\n",
    "sv_prob = bigram_sentence_probability(sv_sentence, word_counts_sv, bigram_counts_sv, len(word_counts_sv))\n",
    "print(f\"Probability of the sentence '{sv_sentence}' in Swedish-English data: {sv_prob}\")\n",
    "\n",
    "# OOV word test\n",
    "oov_sentence = \"this is a quixotic test sentence\"\n",
    "oov_prob_de = bigram_sentence_probability(oov_sentence, word_counts_de, bigram_counts_de, len(word_counts_de))\n",
    "oov_prob_fr = bigram_sentence_probability(oov_sentence, word_counts_fr, bigram_counts_fr, len(word_counts_fr))\n",
    "oov_prob_sv = bigram_sentence_probability(oov_sentence, word_counts_sv, bigram_counts_sv, len(word_counts_sv))\n",
    "print(f\"Probability of the sentence with an OOV word '{oov_sentence}' in German-English data: {oov_prob_de}\")\n",
    "print(f\"Probability of the sentence with an OOV word '{oov_sentence}' in French-English data: {oov_prob_fr}\")\n",
    "print(f\"Probability of the sentence with an OOV word '{oov_sentence}' in Swedish-English data: {oov_prob_sv}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-20T12:03:23.314658300Z",
     "start_time": "2024-02-20T12:03:23.241676400Z"
    }
   },
   "id": "a0e27540712378b5",
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "source": [
    "### (C) Translation Model\n",
    "***"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ce29fab969b8a126"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def create_parallel_sentences(df_en, df_foreign):\n",
    "    return list(zip(df_en, df_foreign))\n",
    "\n",
    "# Create parallel corpora\n",
    "de_parallel_sentences = create_parallel_sentences(de_en_df['English_DE'], de_en_df['German'])\n",
    "fr_parallel_sentences = create_parallel_sentences(fr_en_df['English_FR'], fr_en_df['French'])\n",
    "sv_parallel_sentences = create_parallel_sentences(sv_en_df['English_SV'], sv_en_df['Swedish'])\n",
    "\n",
    "# Select which language pair to use for the IBM Model 1\n",
    "parallel_sentences = de_parallel_sentences\n",
    "\n",
    "# Initialize translation probabilities uniformly\n",
    "def initialize_translation_probabilities(parallel_sentences):\n",
    "    all_en_words = set(word for en, _ in parallel_sentences for word in re.findall(r'\\b\\w+\\b', en.lower()))\n",
    "    all_foreign_words = set(word for _, foreign in parallel_sentences for word in re.findall(r'\\b\\w+\\b', foreign.lower()))\n",
    "\n",
    "    # Add a buffer for the <NULL> token\n",
    "    all_en_words.add('<NULL>')\n",
    "\n",
    "    initial_prob = 1 / (len(all_en_words))  # Now all_en_words includes <NULL>\n",
    "    translation_probs = {f_word: {e_word: initial_prob for e_word in all_en_words} for f_word in all_foreign_words}\n",
    "\n",
    "    return translation_probs\n",
    "\n",
    "def em_algorithm(parallel_sentences, num_iterations=5):\n",
    "    print(\"Initializing translation probabilities...\")\n",
    "    start_time = time.time()\n",
    "    translation_probs = initialize_translation_probabilities(parallel_sentences)\n",
    "    initialization_time = time.time() - start_time\n",
    "    print(f\"Initialization completed in {initialization_time:.2f} seconds.\")\n",
    "\n",
    "    for i in range(num_iterations):\n",
    "        iteration_start_time = time.time()\n",
    "        print(f\"Starting iteration {i+1}/{num_iterations}...\")\n",
    "\n",
    "        count_e_f = defaultdict(lambda: defaultdict(float))\n",
    "        total_e = defaultdict(float)\n",
    "\n",
    "        # E-step\n",
    "        print(\"  E-step...\")\n",
    "        for en_sentence, foreign_sentence in parallel_sentences:\n",
    "            en_words = ['<NULL>'] + re.findall(r'\\b\\w+\\b', en_sentence.lower())\n",
    "            foreign_words = re.findall(r'\\b\\w+\\b', foreign_sentence.lower())\n",
    "            for f_word in foreign_words:\n",
    "                denom_c = sum(translation_probs[f_word][e_word] for e_word in en_words)\n",
    "                for e_word in en_words:\n",
    "                    count_e_f[f_word][e_word] += translation_probs[f_word][e_word] / denom_c\n",
    "                    total_e[e_word] += translation_probs[f_word][e_word] / denom_c\n",
    "\n",
    "        # M-step\n",
    "        print(\"  M-step...\")\n",
    "        for f_word in translation_probs:\n",
    "            for e_word in translation_probs[f_word]:\n",
    "                translation_probs[f_word][e_word] = count_e_f[f_word][e_word] / total_e[e_word]\n",
    "\n",
    "        iteration_end_time = time.time() - iteration_start_time\n",
    "        print(f\"Completed iteration {i+1}/{num_iterations} in {iteration_end_time:.2f} seconds.\")\n",
    "\n",
    "    return translation_probs"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-20T12:03:23.317656200Z",
     "start_time": "2024-02-20T12:03:23.288663800Z"
    }
   },
   "id": "75ae3e47078f3f1",
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting the EM algorithm...\n",
      "Initializing translation probabilities...\n",
      "Initialization completed in 25.01 seconds.\n",
      "Starting iteration 1/5...\n",
      "  E-step...\n",
      "  M-step...\n",
      "Completed iteration 1/5 in 186.56 seconds.\n",
      "Starting iteration 2/5...\n",
      "  E-step...\n",
      "  M-step...\n",
      "Completed iteration 2/5 in 478.42 seconds.\n",
      "Starting iteration 3/5...\n",
      "  E-step...\n",
      "  M-step...\n",
      "Completed iteration 3/5 in 461.66 seconds.\n",
      "Starting iteration 4/5...\n",
      "  E-step...\n",
      "  M-step...\n",
      "Completed iteration 4/5 in 557.25 seconds.\n",
      "Starting iteration 5/5...\n",
      "  E-step...\n",
      "  M-step...\n",
      "Completed iteration 5/5 in 541.42 seconds.\n",
      "EM algorithm completed.\n",
      "Top 10 likely translations for 'european':\n",
      "europäischen: 0.5632761859565995\n",
      "europäische: 0.2475383085188334\n",
      "der: 0.042232841286649406\n",
      "die: 0.027021545239374084\n",
      "union: 0.020943386322625646\n",
      "und: 0.01038648858589231\n",
      "in: 0.009981265254037574\n",
      "den: 0.00832870844471093\n",
      "das: 0.005713162414687346\n",
      "für: 0.004639426042403674\n"
     ]
    }
   ],
   "source": [
    "# Before running the EM algorithm\n",
    "print(\"Starting the EM algorithm...\")\n",
    "translation_probs = em_algorithm(parallel_sentences)\n",
    "print(\"EM algorithm completed.\")\n",
    "\n",
    "# Print the 10 most likely translations for the English word \"european\"\n",
    "english_word = \"european\"\n",
    "likely_translations = [(f_word, translation_probs[f_word][english_word]) for f_word in translation_probs if english_word in translation_probs[f_word]]\n",
    "likely_translations.sort(key=lambda x: x[1], reverse=True)\n",
    "print(\"\\nTop 10 likely translations for 'european':\")\n",
    "for foreign_word, prob in likely_translations[:10]:\n",
    "    print(f\"{foreign_word}: {prob}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-20T12:42:45.647645Z",
     "start_time": "2024-02-20T12:03:23.295665Z"
    }
   },
   "id": "4c8584d6227de328",
   "execution_count": 12
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Self-check: if our goal is to translate from some language into English, why does our conditional probability seem to be written backwards? Why don't we estimate P(e|f) instead?\n",
    "The conditional probability in translation models may appear counterintuitive at first glance. In IBM Model 1, we calculate `P(f|e)` rather than `P(e|f)` because we are establishing an alignment model rather than a direct translation model. This model determines the probability that a word in a foreign language aligns with or translates from an English word. In short we are modeling the generative process of the foreign language given the English text. This approach allows the model to be symmetric and supporting translation in either direction which is beneficial for creating a bilingual model that can assist in translating both to and from English."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1843b1f7d46af262"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### (D) Decoding"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1675b37c1c805994"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translated sentence: reasonably house\n"
     ]
    }
   ],
   "source": [
    "def greedy_decoder(foreign_sentence, translation_probs):\n",
    "    # Tokenize the foreign sentence\n",
    "    # The regular expression \\b\\w+\\b matches whole words\n",
    "    foreign_words = re.findall(r'\\b\\w+\\b', foreign_sentence.lower())\n",
    "    translated_sentence = []\n",
    "\n",
    "    # For each foreign word find the English word with the highest translation probability\n",
    "    for f_word in foreign_words:\n",
    "        best_prob = 0\n",
    "        best_english_word = None\n",
    "        for e_word, prob in translation_probs.get(f_word, {}).items():\n",
    "            if prob > best_prob:\n",
    "                best_prob = prob\n",
    "                best_english_word = e_word\n",
    "        # If no translation is found use the foreign word\n",
    "        translated_sentence.append(best_english_word if best_english_word else f_word)\n",
    "\n",
    "    return ' '.join(translated_sentence)\n",
    "\n",
    "foreign_sentence = \"das Haus\"\n",
    "english_translation = greedy_decoder(foreign_sentence, translation_probs)\n",
    "print(f\"Translated sentence: {english_translation}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-20T12:42:45.768643600Z",
     "start_time": "2024-02-20T12:42:45.667648100Z"
    }
   },
   "id": "579e8c048b451fa4",
   "execution_count": 13
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Simplifying Assumptions:\n",
    "- **Word-by-Word Translation**: This approach assumes that a sentence can be translated word-by-word which rarely is the case given differences in grammar and sentence structure between languages.\n",
    "- **Ignoring Word Order**: The resulting translation does not consider the correct word order in English.\n",
    "- **No Context Consideration**: Each word is translated independently of its context which can lead to incorrect translations for words with multiple meanings.\n",
    "- **Out-of-Vocabulary Words**: Words not seen in the training data will not be translated correctly."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "77f7227e06157f39"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Problem 3: Discussion\n",
    "***\n",
    "\n",
    "#### (A) Propose a number of different evaluation protocols for machine translation systems and discuss their advantages and disadvantages. What does it mean for a translation to be \"good\"? Minimally, you should think of one manual and one automatic procedure. (The point here is not that you should search the web but that you should try to come up with your own ideas.)\n",
    "\n",
    "\n",
    "#### (B) The following example shows a number of sentences automatically translated from Estonian into English. In Estonian, ta means either \"he\" or \"she\", depending on whom we're talking about. Please comment on the translated sentences: what do you think are the technical reasons we see this effect? Do you consider this to be a bug or a feature?\n",
    "\n",
    "#### (c) Below, we consider three sentences that include the English word bat and their automatic translation into Swedish by Google Translate.Why do you think the translation system has been able to select the correct translation of bat in the first two cases? What might be the reason that it has invented a new nonsense word in the third case?\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "abc48290c4db11b3"
  },
  {
   "cell_type": "markdown",
   "id": "88753424",
   "metadata": {},
   "source": [
    "## References\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c902e53e01be6b5",
   "metadata": {
    "id": "3c902e53e01be6b5"
   },
   "source": [
    "## Self Check\n",
    "***\n",
    "- Have you answered all questions to the best of your ability?\n",
    "Yes, we have.\n",
    "- Is all the required information on the front page, is the file name correct etc.?\n",
    "Indeed, all the required information on the front page has been included.\n",
    "- Anything else you can easily check? (details, terminology, arguments, clearly stated answers etc.?)\n",
    "We have checked, and everything looks good."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
