{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0e7365d49bc45f1",
   "metadata": {
    "id": "b0e7365d49bc45f1"
   },
   "source": [
    "# Group 5 - Module 5: Natural Language Processing\n",
    "\n",
    "***\n",
    "### Group Members:\n",
    "* **Nils Dunlop, 20010127-2359, Applied Data Science, e-mail: gusdunlni@student.gu.se (16 hours)**\n",
    "* **Francisco Erazo, 19930613-9214, Applied Data Science, e-mail: guserafr@student.gu.se (16 hours)**\n",
    "\n",
    "#### **We hereby declare that we have both actively participated in solving every exercise. All solutions are entirely our own work, without having taken part of other solutions.\" (This is independent and additional to any declaration that you may encounter in the electronic submission system.)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a40ce4f1465244c7",
   "metadata": {
    "id": "a40ce4f1465244c7"
   },
   "source": [
    "# Assignment 5\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "426429e48a4209ce",
   "metadata": {
    "id": "426429e48a4209ce"
   },
   "source": [
    "## Problem 1: Reading and Reflection\n",
    "***\n",
    "### **(A) AI Problems with a Wide Range of Approaches:**\n",
    "\n",
    "Several AI problems have seen a similarly wide array of approaches, reflecting the many computational techniques utilized over the decades. Some notable examples include:\n",
    "- **Speech Recognition:** From early phonetic-based systems to modern deep learning approaches, speech recognition has evolved significantly. Early systems relied on simple pattern matching and rule-based systems to understand spoken language. Over time, statistical models like Hidden Markov Models (HMMs) played a crucial role, and now deep neural networks, particularly recurrent neural networks (RNNs) and convolutional neural networks (CNNs) dominate the field.\n",
    "   \n",
    "- **Computer Vision:** The evolution of computer vision techniques from simple edge detection algorithms and feature extraction methods to sophisticated deep learning models is another example. Techniques have ranged from geometric model fitting and template matching to the use of deep convolutional neural networks (CNNs) for tasks such as image classification, object detection, and semantic segmentation.\n",
    "   \n",
    "- **Game Playing:** The development of AI for game playing, from chess and checkers to complex video games, has seen strategies evolve from brute-force search algorithms and heuristic-based approaches to the use of machine learning techniques, including reinforcement learning and deep learning for strategy optimization.\n",
    "\n",
    "### **(B) Similarities Between Rule-based Translation Systems and Neural Systems:**\n",
    "\n",
    "Despite their differences, there are notable similarities between some aspects of rule-based translation systems and state-of-the-art neural systems:\n",
    "\n",
    "- **Structured Knowledge:** Rule-based systems are explicitly programmed with linguistic rules and dictionaries. Neural systems, particularly those employing attention mechanisms, implicitly learn to encode structured knowledge about languages through training on large datasets, creating internal representations that mirror some aspects of linguistic rules.\n",
    "\n",
    "- **Context Handling:** Both systems attempt to handle context in translation, albeit in radically different ways. Rule-based systems may use context rules to choose the correct translation of a word based on surrounding words. Neural systems, especially those using mechanisms like attention, are able to consider broader context in a more fluid and dynamic manner to understand the appropriate meaning.\n",
    "   \n",
    "- **Error Correction:** Both systems have mechanisms for error correction, although neural systems do this implicitly. Rule-based systems may use post-processing rules to correct common mistakes, while neural systems learn error patterns and their corrections during training.\n",
    "\n",
    "### **(C) Situations Favoring Rule-based Solutions Over Modern Approaches:**\n",
    "\n",
    "There are scenarios where a rule-based solution might be preferable to a modern neural or statistical approach:\n",
    "\n",
    "- **Limited Data Scenarios:** For languages or specialized domains where there is a scarcity of training data, rule-based systems can be more practical and effective because they do not require large datasets to perform reasonably well.\n",
    "   \n",
    "- **Predictability and Transparency:** In situations where predictability, transparency, and the ability to audit or explain decisions are crucial (e.g., in some legal or regulatory contexts), rule-based systems offer clear advantages. Their decisions are based on explicit rules that can be examined, understood, and modified by humans.\n",
    "\n",
    "- **Real-time Constraints:** In cases where computational resources are limited or real-time performance is essential, the relative simplicity and lower computational requirements of rule-based systems can be an advantage over more resource-intensive neural models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1b6dbabbbd02376",
   "metadata": {
    "id": "b1b6dbabbbd02376"
   },
   "source": [
    "## Problem 2: Implementation\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2344500c97686b5",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### (A) Warmup: Word Frequencies\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "817613a5b9b4d9cb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-20T12:03:20.955677100Z",
     "start_time": "2024-02-20T12:03:16.577814600Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "read_csv() got an unexpected keyword argument 'squeeze'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 10\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtime\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Load datasets into DataFrames\u001b[39;00m\n\u001b[1;32m      9\u001b[0m de_en_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame({\n\u001b[0;32m---> 10\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGerman\u001b[39m\u001b[38;5;124m'\u001b[39m: pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdat410_europarl/europarl-v7.de-en.lc.de\u001b[39m\u001b[38;5;124m\"\u001b[39m, sep\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m, header\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, names\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m], squeeze\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m),\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEnglish_DE\u001b[39m\u001b[38;5;124m'\u001b[39m: pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdat410_europarl/europarl-v7.de-en.lc.en\u001b[39m\u001b[38;5;124m\"\u001b[39m, sep\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m, header\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, names\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m], squeeze\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     12\u001b[0m })\n\u001b[1;32m     14\u001b[0m fr_en_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame({\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFrench\u001b[39m\u001b[38;5;124m'\u001b[39m: pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdat410_europarl/europarl-v7.fr-en.lc.fr\u001b[39m\u001b[38;5;124m\"\u001b[39m, sep\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m, header\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, names\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m], squeeze\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m),\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEnglish_FR\u001b[39m\u001b[38;5;124m'\u001b[39m: pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdat410_europarl/europarl-v7.fr-en.lc.en\u001b[39m\u001b[38;5;124m\"\u001b[39m, sep\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m, header\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, names\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m], squeeze\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     17\u001b[0m })\n\u001b[1;32m     19\u001b[0m sv_en_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame({\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSwedish\u001b[39m\u001b[38;5;124m'\u001b[39m: pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdat410_europarl/europarl-v7.sv-en.lc.sv\u001b[39m\u001b[38;5;124m\"\u001b[39m, sep\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m, header\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, names\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m], squeeze\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m),\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEnglish_SV\u001b[39m\u001b[38;5;124m'\u001b[39m: pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdat410_europarl/europarl-v7.sv-en.lc.en\u001b[39m\u001b[38;5;124m\"\u001b[39m, sep\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m, header\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, names\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m], squeeze\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     22\u001b[0m })\n",
      "\u001b[0;31mTypeError\u001b[0m: read_csv() got an unexpected keyword argument 'squeeze'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import re\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import time\n",
    "\n",
    "# Load datasets into DataFrames\n",
    "# The .squeeze() method is used to convert the single-column DataFrames into Series\n",
    "de_en_df = pd.DataFrame({\n",
    "    'German': pd.read_csv(\"dat410_europarl/europarl-v7.de-en.lc.de\", sep='\\n', header=None, names=['text']).squeeze(),\n",
    "    'English_DE': pd.read_csv(\"dat410_europarl/europarl-v7.de-en.lc.en\", sep='\\n', header=None, names=['text']).squeeze()\n",
    "})\n",
    "\n",
    "fr_en_df = pd.DataFrame({\n",
    "    'French': pd.read_csv(\"dat410_europarl/europarl-v7.fr-en.lc.fr\", sep='\\n', header=None, names=['text']).squeeze(),\n",
    "    'English_FR': pd.read_csv(\"dat410_europarl/europarl-v7.fr-en.lc.en\", sep='\\n', header=None, names=['text']).squeeze()\n",
    "})\n",
    "\n",
    "sv_en_df = pd.DataFrame({\n",
    "    'Swedish': pd.read_csv(\"dat410_europarl/europarl-v7.sv-en.lc.sv\", sep='\\n', header=None, names=['text']).squeeze(),\n",
    "    'English_SV': pd.read_csv(\"dat410_europarl/europarl-v7.sv-en.lc.en\", sep='\\n', header=None, names=['text']).squeeze()\n",
    "})\n",
    "\n",
    "# Display the first few rows of the German-English dataset\n",
    "print(de_en_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "843e900b35ca21fe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-20T12:03:20.974666300Z",
     "start_time": "2024-02-20T12:03:20.906664800Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              French  \\\n",
      "0  je déclare reprise la session du parlement eur...   \n",
      "1  comme vous avez pu le constater , le grand &qu...   \n",
      "2  vous avez souhaité un débat à ce sujet dans le...   \n",
      "3  en attendant , je souhaiterais , comme un cert...   \n",
      "4  je vous invite à vous lever pour cette minute ...   \n",
      "\n",
      "                                          English_FR  \n",
      "0  i declare resumed the session of the european ...  \n",
      "1  although , as you will have seen , the dreaded...  \n",
      "2  you have requested a debate on this subject in...  \n",
      "3  in the meantime , i should like to observe a m...  \n",
      "4  please rise , then , for this minute &apos; s ...  \n"
     ]
    }
   ],
   "source": [
    "# Display the first few rows of the French-English dataset\n",
    "print(fr_en_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d02f8867e40f6f4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-20T12:03:21.121661100Z",
     "start_time": "2024-02-20T12:03:20.983663500Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             Swedish  \\\n",
      "0  jag förklarar europaparlamentets session återu...   \n",
      "1  som ni kunnat konstatera ägde &quot; den stora...   \n",
      "2  ni har begärt en debatt i ämnet under sammantr...   \n",
      "3  till dess vill jag att vi , som ett antal koll...   \n",
      "4             jag ber er resa er för en tyst minut .   \n",
      "\n",
      "                                          English_SV  \n",
      "0  i declare resumed the session of the european ...  \n",
      "1  although , as you will have seen , the dreaded...  \n",
      "2  you have requested a debate on this subject in...  \n",
      "3  in the meantime , i should like to observe a m...  \n",
      "4  please rise , then , for this minute &apos; s ...  \n"
     ]
    }
   ],
   "source": [
    "# Display the first few rows of the Swedish-English dataset\n",
    "print(sv_en_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3c9bd61df230e8d2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-20T12:03:21.153668500Z",
     "start_time": "2024-02-20T12:03:21.107656300Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Function to calculate word frequencies\n",
    "def calculate_word_frequencies(df_column):\n",
    "    word_counts = Counter()\n",
    "    for text in df_column:\n",
    "        # Remove punctuation and other non-word characters\n",
    "        words = re.findall(r'\\b\\w+\\b', text.lower())\n",
    "        word_counts.update(words)\n",
    "    return word_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "adcf00330048fa75",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-20T12:03:22.188657600Z",
     "start_time": "2024-02-20T12:03:21.174659800Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Calculate word frequencies for each language\n",
    "de_word_counts = calculate_word_frequencies(de_en_df['German'])\n",
    "en_de_word_counts = calculate_word_frequencies(de_en_df['English_DE'])\n",
    "\n",
    "fr_word_counts = calculate_word_frequencies(fr_en_df['French'])\n",
    "en_fr_word_counts = calculate_word_frequencies(fr_en_df['English_FR'])\n",
    "\n",
    "sv_word_counts = calculate_word_frequencies(sv_en_df['Swedish'])\n",
    "en_sv_word_counts = calculate_word_frequencies(sv_en_df['English_SV'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c66efd3074499963",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-20T12:03:22.190662Z",
     "start_time": "2024-02-20T12:03:22.178656600Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Function to print most common words\n",
    "def print_most_common(word_counts, language, num=10):\n",
    "    print(f\"Most common words in {language}:\")\n",
    "    for word, count in word_counts.most_common(num):\n",
    "        print(f\"{word}: {count}\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1c678acf838a13c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-20T12:03:22.230660500Z",
     "start_time": "2024-02-20T12:03:22.189653900Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most common words in German:\n",
      "die: 10521\n",
      "der: 9374\n",
      "und: 7028\n",
      "in: 4175\n",
      "zu: 3169\n",
      "den: 2976\n",
      "wir: 2863\n",
      "daß: 2738\n",
      "ich: 2670\n",
      "das: 2669\n",
      "\n",
      "\n",
      "Most common words in English (DE):\n",
      "the: 19853\n",
      "of: 9633\n",
      "to: 9069\n",
      "and: 7307\n",
      "in: 6278\n",
      "is: 4478\n",
      "that: 4441\n",
      "a: 4438\n",
      "we: 3372\n",
      "this: 3362\n",
      "\n",
      "\n",
      "Most common words in French:\n",
      "apos: 16729\n",
      "de: 14528\n",
      "la: 9746\n",
      "et: 6620\n",
      "l: 6536\n",
      "le: 6177\n",
      "à: 5588\n",
      "les: 5587\n",
      "des: 5232\n",
      "que: 4797\n",
      "\n",
      "\n",
      "Most common words in English (FR):\n",
      "the: 19627\n",
      "of: 9534\n",
      "to: 8992\n",
      "and: 7214\n",
      "in: 6197\n",
      "is: 4453\n",
      "that: 4421\n",
      "a: 4388\n",
      "we: 3341\n",
      "this: 3332\n",
      "\n",
      "\n",
      "Most common words in Swedish:\n",
      "att: 9181\n",
      "och: 7038\n",
      "i: 5954\n",
      "det: 5687\n",
      "som: 5028\n",
      "för: 4959\n",
      "av: 4013\n",
      "är: 3840\n",
      "en: 3724\n",
      "vi: 3211\n",
      "\n",
      "\n",
      "Most common words in English (SV):\n",
      "the: 19327\n",
      "of: 9344\n",
      "to: 8814\n",
      "and: 6949\n",
      "in: 6124\n",
      "is: 4400\n",
      "that: 4357\n",
      "a: 4271\n",
      "we: 3223\n",
      "this: 3222\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print the 10 most frequent words for each language\n",
    "print_most_common(de_word_counts, \"German\")\n",
    "print_most_common(en_de_word_counts, \"English (DE)\")\n",
    "\n",
    "print_most_common(fr_word_counts, \"French\")\n",
    "print_most_common(en_fr_word_counts, \"English (FR)\")\n",
    "\n",
    "print_most_common(sv_word_counts, \"Swedish\")\n",
    "print_most_common(en_sv_word_counts, \"English (SV)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2616bdd5e8d0c0ba",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-20T12:03:22.255652500Z",
     "start_time": "2024-02-20T12:03:22.227664300Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability of 'speaker' in German: 0.000000\n",
      "Probability of 'zebra' in German: 0.000000\n",
      "Probability of 'speaker' in English (DE): 0.000042\n",
      "Probability of 'zebra' in English (DE): 0.000000\n",
      "Probability of 'speaker' in French: 0.000000\n",
      "Probability of 'zebra' in French: 0.000000\n",
      "Probability of 'speaker' in English (FR): 0.000046\n",
      "Probability of 'zebra' in English (FR): 0.000000\n",
      "Probability of 'speaker' in Swedish: 0.000000\n",
      "Probability of 'zebra' in Swedish: 0.000000\n",
      "Probability of 'speaker' in English (SV): 0.000039\n",
      "Probability of 'zebra' in English (SV): 0.000000\n"
     ]
    }
   ],
   "source": [
    "def print_probabilities(word_counts, language, total_words):\n",
    "    for word in ['speaker', 'zebra']:\n",
    "        if word in word_counts:\n",
    "            prob = word_counts[word] / total_words\n",
    "        else:\n",
    "            prob = 0\n",
    "        print(f\"Probability of '{word}' in {language}: {prob:.6f}\")\n",
    "\n",
    "total_de_words = sum(de_word_counts.values())\n",
    "total_en_de_words = sum(en_de_word_counts.values())\n",
    "total_fr_words = sum(fr_word_counts.values())\n",
    "total_en_fr_words = sum(en_fr_word_counts.values())\n",
    "total_sv_words = sum(sv_word_counts.values())\n",
    "total_en_sv_words = sum(en_sv_word_counts.values())\n",
    "\n",
    "print_probabilities(de_word_counts, \"German\", total_de_words)\n",
    "print_probabilities(en_de_word_counts, \"English (DE)\", total_en_de_words)\n",
    "print_probabilities(fr_word_counts, \"French\", total_fr_words)\n",
    "print_probabilities(en_fr_word_counts, \"English (FR)\", total_en_fr_words)\n",
    "print_probabilities(sv_word_counts, \"Swedish\", total_sv_words)\n",
    "print_probabilities(en_sv_word_counts, \"English (SV)\", total_en_sv_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f342dac",
   "metadata": {},
   "source": [
    "The output indicates that the word 'speaker' is found in the English versions of the texts, specifically English (DE), English (FR), and English (SV), with respective probabilities of 0.000042, 0.000046, and 0.000039. This suggests a limited presence of 'speaker' only within these English translations. In contrast, the absence of 'speaker' in German, French, and Swedish, as shown by a 0 probability, confirms its unavailability in these languages.\n",
    "\n",
    "Regarding the word 'zebra', its 0 probability across all examined languages implies that it does not feature in any of the texts. This reveals that 'speaker' is present solely in the English corpora, while 'zebra' is not found in any of the languages analyzed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "603802a163b48ed7",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### (B) Language Modeling\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b3af9522c34a79e7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-20T12:03:23.261657700Z",
     "start_time": "2024-02-20T12:03:22.252666600Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Tokenize the text and add start and end tokens\n",
    "# The words between the start and end markers are determined and tokenized\n",
    "tokenized_text_de = [('<START>',) + tuple(re.findall(r'\\b\\w+\\b', sentence.lower())) + ('<END>',) for sentence in de_en_df['English_DE']]\n",
    "tokenized_text_fr = [('<START>',) + tuple(re.findall(r'\\b\\w+\\b', sentence.lower())) + ('<END>',) for sentence in fr_en_df['English_FR']]\n",
    "tokenized_text_sv = [('<START>',) + tuple(re.findall(r'\\b\\w+\\b', sentence.lower())) + ('<END>',) for sentence in sv_en_df['English_SV']]\n",
    "\n",
    "# Count individual words and bigrams\n",
    "word_counts_de = Counter()\n",
    "bigram_counts_de = Counter()\n",
    "word_counts_fr = Counter()\n",
    "bigram_counts_fr = Counter()\n",
    "word_counts_sv = Counter()\n",
    "bigram_counts_sv = Counter()\n",
    "\n",
    "# Update word counts and form bigram tuples\n",
    "for sentence in tokenized_text_de:\n",
    "    word_counts_de.update(sentence)\n",
    "    bigram_counts_de.update(zip(sentence[:-1], sentence[1:]))\n",
    "\n",
    "for sentence in tokenized_text_fr:\n",
    "    word_counts_fr.update(sentence)\n",
    "    bigram_counts_fr.update(zip(sentence[:-1], sentence[1:]))\n",
    "\n",
    "for sentence in tokenized_text_sv:\n",
    "    word_counts_sv.update(sentence)\n",
    "    bigram_counts_sv.update(zip(sentence[:-1], sentence[1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a0e27540712378b5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-20T12:03:23.314658300Z",
     "start_time": "2024-02-20T12:03:23.241676400Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'word_counts_de' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 22\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Test the bigram_sentence_probability for the German-English dataset\u001b[39;00m\n\u001b[1;32m     21\u001b[0m de_sentence \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe economic impact of the legislation was significant.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 22\u001b[0m de_prob \u001b[38;5;241m=\u001b[39m bigram_sentence_probability(de_sentence, word_counts_de, bigram_counts_de, \u001b[38;5;28mlen\u001b[39m(word_counts_de))\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProbability of the sentence \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mde_sentence\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m in German-English data: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mde_prob\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# Test the bigram_sentence_probability for the French-English dataset\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'word_counts_de' is not defined"
     ]
    }
   ],
   "source": [
    "# Compute the probability of a sentence using a bigram model with Laplace smoothing\n",
    "def bigram_sentence_probability(sentence, word_counts, bigram_counts, total_words, smoothing=1):\n",
    "    sentence = ('<START>',) + tuple(re.findall(r'\\b\\w+\\b', sentence.lower())) + ('<END>',)\n",
    "    bigram_probs = []\n",
    "\n",
    "    for first_word, second_word in zip(sentence[:-1], sentence[1:]):\n",
    "        bigram_count = bigram_counts[(first_word, second_word)]\n",
    "        word_count = word_counts[first_word]\n",
    "        \n",
    "        # Apply Laplace smoothing\n",
    "        prob = (bigram_count + smoothing) / (word_count + smoothing * (total_words + 1))\n",
    "        bigram_probs.append(prob)\n",
    "\n",
    "    # Logarithms is used to avoid underflow with long sentences\n",
    "    log_probs = np.log(bigram_probs)\n",
    "    log_prob_sentence = np.sum(log_probs)\n",
    "\n",
    "    return np.exp(log_prob_sentence)\n",
    "\n",
    "# Test the bigram_sentence_probability for the German-English dataset\n",
    "de_sentence = \"The economic impact of the legislation was significant.\"\n",
    "de_prob = bigram_sentence_probability(de_sentence, word_counts_de, bigram_counts_de, len(word_counts_de))\n",
    "print(f\"Probability of the sentence '{de_sentence}' in German-English data: {de_prob}\")\n",
    "\n",
    "# Test the bigram_sentence_probability for the French-English dataset\n",
    "fr_sentence = \"Diplomatic efforts were intensified to resolve the conflict.\"\n",
    "fr_prob = bigram_sentence_probability(fr_sentence, word_counts_fr, bigram_counts_fr, len(word_counts_fr))\n",
    "print(f\"Probability of the sentence '{fr_sentence}' in French-English data: {fr_prob}\")\n",
    "\n",
    "# Test the bigram_sentence_probability for the Swedish-English dataset\n",
    "sv_sentence = \"Environmental policies have become increasingly important.\"\n",
    "sv_prob = bigram_sentence_probability(sv_sentence, word_counts_sv, bigram_counts_sv, len(word_counts_sv))\n",
    "print(f\"Probability of the sentence '{sv_sentence}' in Swedish-English data: {sv_prob}\")\n",
    "\n",
    "# OOV word test\n",
    "oov_sentence = \"this is a quixotic test sentence\"\n",
    "oov_prob_de = bigram_sentence_probability(oov_sentence, word_counts_de, bigram_counts_de, len(word_counts_de))\n",
    "oov_prob_fr = bigram_sentence_probability(oov_sentence, word_counts_fr, bigram_counts_fr, len(word_counts_fr))\n",
    "oov_prob_sv = bigram_sentence_probability(oov_sentence, word_counts_sv, bigram_counts_sv, len(word_counts_sv))\n",
    "print(f\"Probability of the sentence with an OOV word '{oov_sentence}' in German-English data: {oov_prob_de}\")\n",
    "print(f\"Probability of the sentence with an OOV word '{oov_sentence}' in French-English data: {oov_prob_fr}\")\n",
    "print(f\"Probability of the sentence with an OOV word '{oov_sentence}' in Swedish-English data: {oov_prob_sv}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Laplace smoothing is used in the cases in which we compute the probability of a sentence that contains a word that is not in the texts, in this case 'quixotic'. Laplace smoothing ensures that every possible word and word pair has a non-zero probability, even if it hasn't been seen in the training data. This means that the word 'quixotic' is assigned with a small, non-zero probability, which helps prevent the model from assigning a zero probability to sentences with words that were not present in the training corpus.\n",
    "\n",
    "Something, that we noticed in our test before is that the lenght of the sentences affect the probability because of bigram frenquency combinations. So, for very long sentences, it is possible that the probability is very low even if the sentence is in the language. We tested with a sentence of 100 words and the probability was 0. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce29fab969b8a126",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### (C) Translation Model\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "75ae3e47078f3f1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-20T12:03:23.317656200Z",
     "start_time": "2024-02-20T12:03:23.288663800Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_parallel_sentences(df_en, df_foreign):\n",
    "    return list(zip(df_en, df_foreign))\n",
    "\n",
    "# Create parallel corpora\n",
    "de_parallel_sentences = create_parallel_sentences(de_en_df['English_DE'], de_en_df['German'])\n",
    "fr_parallel_sentences = create_parallel_sentences(fr_en_df['English_FR'], fr_en_df['French'])\n",
    "sv_parallel_sentences = create_parallel_sentences(sv_en_df['English_SV'], sv_en_df['Swedish'])\n",
    "\n",
    "# Select which language pair to use for the IBM Model 1\n",
    "# We decided to look into English and German parallel sentences\n",
    "parallel_sentences = de_parallel_sentences\n",
    "\n",
    "# Initialize translation probabilities uniformly\n",
    "def initialize_translation_probabilities(parallel_sentences):\n",
    "    all_en_words = set()\n",
    "    all_foreign_words = set()\n",
    "\n",
    "    # Collect English and foreign words from the parallel sentences\n",
    "    for en, foreign in parallel_sentences:\n",
    "        en_words = re.findall(r'\\b\\w+\\b', en.lower())\n",
    "        foreign_words = re.findall(r'\\b\\w+\\b', foreign.lower())\n",
    "        all_en_words.update(en_words)\n",
    "        all_foreign_words.update(foreign_words)\n",
    "\n",
    "    # Add a buffer for the <NULL> token\n",
    "    all_en_words.add('<NULL>')\n",
    "\n",
    "    # Calculate the initial probability\n",
    "    initial_prob = 1 / len(all_en_words)\n",
    "\n",
    "    # Create the translation probabilities dictionary\n",
    "    translation_probs = {}\n",
    "    for f_word in all_foreign_words:\n",
    "        translation_probs[f_word] = {}\n",
    "        for e_word in all_en_words:\n",
    "            translation_probs[f_word][e_word] = initial_prob\n",
    "\n",
    "    return translation_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d3f931b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def em_algorithm(parallel_sentences, num_iterations=5):\n",
    "    print(\"Initializing translation probabilities...\")\n",
    "    start_time = time.time()\n",
    "    translation_probs = initialize_translation_probabilities(parallel_sentences)\n",
    "    initialization_time = time.time() - start_time\n",
    "    print(f\"Initialization completed in {initialization_time:.2f} seconds.\")\n",
    "\n",
    "    for i in range(num_iterations):\n",
    "        iteration_start_time = time.time()\n",
    "        print(f\"Starting iteration {i+1}/{num_iterations}...\")\n",
    "\n",
    "        count_e_f = defaultdict(lambda: defaultdict(float))\n",
    "        total_e = defaultdict(float)\n",
    "\n",
    "        # E-step\n",
    "        print(\"  E-step...\")\n",
    "        for en_sentence, foreign_sentence in parallel_sentences:\n",
    "            en_words = ['<NULL>'] + re.findall(r'\\b\\w+\\b', en_sentence.lower())\n",
    "            foreign_words = re.findall(r'\\b\\w+\\b', foreign_sentence.lower())\n",
    "            for f_word in foreign_words:\n",
    "                denom_c = 0\n",
    "                for e_word in en_words:\n",
    "                    denom_c += translation_probs[f_word][e_word]\n",
    "                for e_word in en_words:\n",
    "                    delta = translation_probs[f_word][e_word] / denom_c\n",
    "                    count_e_f[f_word][e_word] += delta\n",
    "                    total_e[e_word] += delta\n",
    "\n",
    "        # M-step\n",
    "        print(\"  M-step...\")\n",
    "        for f_word, e_words_probs in translation_probs.items():\n",
    "            for e_word in e_words_probs:\n",
    "                e_word_total = total_e[e_word]\n",
    "                if e_word_total > 0:\n",
    "                    translation_probs[f_word][e_word] = count_e_f[f_word][e_word] / e_word_total\n",
    "\n",
    "        iteration_end_time = time.time() - iteration_start_time\n",
    "        print(f\"Completed iteration {i+1}/{num_iterations} in {iteration_end_time:.2f} seconds.\")\n",
    "\n",
    "    return translation_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4c8584d6227de328",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-20T12:42:45.647645Z",
     "start_time": "2024-02-20T12:03:23.295665Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting the EM algorithm...\n",
      "Initializing translation probabilities...\n",
      "Initialization completed in 25.01 seconds.\n",
      "Starting iteration 1/5...\n",
      "  E-step...\n",
      "  M-step...\n",
      "Completed iteration 1/5 in 186.56 seconds.\n",
      "Starting iteration 2/5...\n",
      "  E-step...\n",
      "  M-step...\n",
      "Completed iteration 2/5 in 478.42 seconds.\n",
      "Starting iteration 3/5...\n",
      "  E-step...\n",
      "  M-step...\n",
      "Completed iteration 3/5 in 461.66 seconds.\n",
      "Starting iteration 4/5...\n",
      "  E-step...\n",
      "  M-step...\n",
      "Completed iteration 4/5 in 557.25 seconds.\n",
      "Starting iteration 5/5...\n",
      "  E-step...\n",
      "  M-step...\n",
      "Completed iteration 5/5 in 541.42 seconds.\n",
      "EM algorithm completed.\n",
      "Top 10 likely translations for 'european':\n",
      "europäischen: 0.5632761859565995\n",
      "europäische: 0.2475383085188334\n",
      "der: 0.042232841286649406\n",
      "die: 0.027021545239374084\n",
      "union: 0.020943386322625646\n",
      "und: 0.01038648858589231\n",
      "in: 0.009981265254037574\n",
      "den: 0.00832870844471093\n",
      "das: 0.005713162414687346\n",
      "für: 0.004639426042403674\n"
     ]
    }
   ],
   "source": [
    "# Before running the EM algorithm\n",
    "print(\"Starting the EM algorithm...\")\n",
    "translation_probs = em_algorithm(parallel_sentences)\n",
    "print(\"EM algorithm completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "360dfd070b0f1ee5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-20T12:48:17.393394900Z",
     "start_time": "2024-02-20T12:48:17.331375400Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 10 likely translations for 'european':\n",
      "europäischen: 0.5632761859565995\n",
      "europäische: 0.2475383085188334\n",
      "der: 0.042232841286649406\n",
      "die: 0.027021545239374084\n",
      "union: 0.020943386322625646\n",
      "und: 0.01038648858589231\n",
      "in: 0.009981265254037574\n",
      "den: 0.00832870844471093\n",
      "das: 0.005713162414687346\n",
      "für: 0.004639426042403674\n"
     ]
    }
   ],
   "source": [
    "# Print the 10 most likely translations for the English word \"european\"\n",
    "english_word = \"european\"\n",
    "likely_translations = []\n",
    "\n",
    "# Collecting all possible translations for the English word\n",
    "for f_word in translation_probs:\n",
    "    if english_word in translation_probs[f_word]:\n",
    "        translation_prob = translation_probs[f_word][english_word]\n",
    "        likely_translations.append((f_word, translation_prob))\n",
    "\n",
    "# Sorting the translations by probability in descending order\n",
    "likely_translations.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Printing the top 10 likely translations\n",
    "print(\"\\nTop 10 likely translations for 'european':\")\n",
    "for foreign_word, prob in likely_translations[:10]:\n",
    "    print(f\"{foreign_word}: {prob}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe9ac3ca",
   "metadata": {},
   "source": [
    "The output from the machine translation model reveals the top ten probable translations for the English word \"european\". Leading the list are \"europäischen\" and \"europäische\" which reflects a strong alignment with the English adjective form. Interestingly, common German articles and prepositions like \"der\", \"die\" and \"in\" also appear suggesting some noise in the model potentially due to their high frequency in the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1843b1f7d46af262",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Self-check: if our goal is to translate from some language into English, why does our conditional probability seem to be written backwards? Why don't we estimate P(e|f) instead?\n",
    "The conditional probability in translation models may appear counterintuitive at first glance. In IBM Model 1, we calculate `P(f|e)` rather than `P(e|f)` because we are establishing an alignment model rather than a direct translation model. This model determines the probability that a word in a foreign language aligns with or translates from an English word. In short we are modeling the generative process of the foreign language given the English text. This approach allows the model to be symmetric and supporting translation in either direction which is beneficial for creating a bilingual model that can assist in translating both to and from English."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1675b37c1c805994",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### (D) Decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "579e8c048b451fa4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-20T12:49:31.053554800Z",
     "start_time": "2024-02-20T12:49:31.019538900Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translated sentence: reasonably house\n",
      "Translated sentence: i speaking colourful bisschen deutsch\n"
     ]
    }
   ],
   "source": [
    "def greedy_decoder(foreign_sentence, translation_probs):\n",
    "    # Tokenize the foreign sentence\n",
    "    # The regular expression \\b\\w+\\b matches whole words\n",
    "    foreign_words = re.findall(r'\\b\\w+\\b', foreign_sentence.lower())\n",
    "    translated_sentence = []\n",
    "\n",
    "    # For each foreign word find the English word with the highest translation probability\n",
    "    for f_word in foreign_words:\n",
    "        best_prob = 0\n",
    "        best_english_word = None\n",
    "        for e_word, prob in translation_probs.get(f_word, {}).items():\n",
    "            if prob > best_prob:\n",
    "                best_prob = prob\n",
    "                best_english_word = e_word\n",
    "\n",
    "        # If no translation is found use the foreign word\n",
    "        if best_english_word:\n",
    "            translated_sentence.append(best_english_word)\n",
    "        else:\n",
    "            translated_sentence.append(f_word)\n",
    "\n",
    "    return ' '.join(translated_sentence)\n",
    "\n",
    "foreign_sentence_1 = \"das Haus\" # The house\n",
    "foreign_sentence_2 = \"Ich spreche ein bisschen Deutsch.\" # I speak a little German.\n",
    "english_translation_1 = greedy_decoder(foreign_sentence_1, translation_probs)\n",
    "english_translation_2 = greedy_decoder(foreign_sentence_2, translation_probs)\n",
    "print(f\"Translated sentence: {english_translation_1}\")\n",
    "print(f\"Translated sentence: {english_translation_2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f7227e06157f39",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Simplifying Assumptions:\n",
    "- **Word-by-Word Translation**: This approach assumes that a sentence can be translated word-by-word which rarely is the case given differences in grammar and sentence structure between languages.\n",
    "- **Ignoring Word Order**: The resulting translation does not consider the correct word order in English.\n",
    "- **No Context Consideration**: Each word is translated independently of its context which can lead to incorrect translations for words with multiple meanings.\n",
    "- **Out-of-Vocabulary Words**: Words not seen in the training data will not be translated correctly.\n",
    "\n",
    "When analyzing the output, it is clear that the method used for translation is quite rudimentary and that the results are imperfect. These translations serve as a starting point and highlight areas where more sophisticated models and techniques are necessary for accurate translation.\n",
    "\n",
    "Finding the English sentence that has the highest probability given a source-language sentence is algorithmically challenging due to the vast number of possible translations and the complexity of the translation model. \n",
    "- The search space for the most probable English sentence is enormous and the model must consider all possible translations and alignments to find the most probable sentence. This process is computationally intensive and requires sophisticated algorithms to efficiently search the space of possible translations. \n",
    "- The probability P(E∣F) depends on both the local context (e.g., word order, grammar) and the broader context (e.g., the sentence's topic or style). Accurately modeling these dependencies requires complex probabilistic models.\n",
    "- Evaluating P(E∣F) for every possible translation E is computationally intensive, especially for long sentences or large vocabularies.\n",
    "- Words can have multiple meanings depending on the context, making it difficult to select the correct translation without understanding the entire sentence.\n",
    "- Phrases that do not translate directly between languages pose a particular challenge, as they require understanding beyond the word level."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abc48290c4db11b3",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Problem 3: Discussion\n",
    "***\n",
    "\n",
    "#### (A) Propose a number of different evaluation protocols for machine translation systems and discuss their advantages and disadvantages. What does it mean for a translation to be \"good\"? Minimally, you should think of one manual and one automatic procedure. (The point here is not that you should search the web but that you should try to come up with your own ideas.)\n",
    "The evaluation of machine translation systems can encompass methods of **Human Evaluation** and **Automatic Evaluation**.\n",
    "\n",
    "**Human Evaluation** might include protocols such as:\n",
    "- **Advantages**: This method provides a nuanced understanding that encompasses assessments of readability, fluency, and the conveyance of original meaning.\n",
    "- **Disadvantages**: However, it is often time-consuming and costly with a susceptibility to human bias.\n",
    "- **Protocols**: \n",
    "  - **Ranking/Scoring**: In this protocol, experts assign scores to translations based on criteria including fluency, adequacy and overall quality.\n",
    "  - **Post-editing Analysis**: This involves an examination of the editing effort necessary to refine machine translations to acceptable standards.\n",
    "\n",
    "**Automatic Evaluation** offers different advantages and follows distinct protocols:\n",
    "- **Advantages**: It is recognized for its speed and cost-effectiveness while providing consistent and repeatable metrics.\n",
    "- **Disadvantages**: The limitation lies in its potential failure to fully grasp semantic correctness and the nuances of fluency.\n",
    "- **Protocols**: \n",
    "  - **BLEU Score**: This widely-used metric compares the n-gram overlap of the machines output with that of a reference translation.\n",
    "  - **TER**: The Translation Edit Rate measures the number of edits required to change a machine-generated translation into an acceptable version based on a reference.\n",
    "\n",
    "Determining a \"good\" translation involves considering accuracy in reflecting the original meaning, maintaining the source's tone, and ensuring fluency within the target language's context. Both human and automatic methods strive to measure these attributes and both have their own degree of efficacy.\n",
    "\n",
    "#### (B) The following example shows a number of sentences automatically translated from Estonian into English. In Estonian, ta means either \"he\" or \"she\", depending on whom we're talking about. Please comment on the translated sentences: what do you think are the technical reasons we see this effect? Do you consider this to be a bug or a feature?\n",
    "The example illustrates that the translation system has chosen \"he\" for professions typically perceived as male-dominated (\"doctor\", \"computer programmer\") and \"she\" for roles often stereotypically female (\"nurse\", babysitter\").\n",
    "\n",
    "This effect likely arises from the training data bias, where the model has learned correlations between gender and profession based on the predominance of these associations in the dataset. While some may view this as a reflection of current societal roles and therefore a feature, it also reinforces gender stereotypes, which can be seen as a bug. As a result, this highlights the need for careful consideration in how translation systems handle gender by potentially providing gender-neutral options or allowing users to specify gender where relevant.\n",
    "\n",
    "#### (c) Below, we consider three sentences that include the English word bat and their automatic translation into Swedish by Google Translate. Why do you think the translation system has been able to select the correct translation of bat in the first two cases? What might be the reason that it has invented a new nonsense word in the third case?\n",
    "When examining the automatic translation of sentences containing the word \"bat\" from English to Swedish via Google Translate we find two successful translations and one anomaly. The first two sentences which reference \"bat\" as an animal are correctly translated due to distinct contextual clues phrases like \"hit the ball\" and \"eats insects\" guide the translation system to the zoological meaning. However, the third sentence presents a peculiar case where the translation system generates a nonsensical term.\n",
    "\n",
    "This anomaly arises because the word \"bat\" can contextually fit both as an animal in the woods and as a piece of sports equipment made of wood. The ambiguity here stems from the translation model's inability to distinguish the term \"bat\" without a clear contextual indicator favoring the sports equipment interpretation. The model defaults to a nonsensical translation which underscores the critical role of context in accurate translation and the challenges faced by models relying solely on word-level data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88753424",
   "metadata": {},
   "source": [
    "## References\n",
    "***\n",
    "\n",
    "- Brownlee, J. (2019). A Gentle Introduction to Expectation-Maximization (EM Algorithm) - MachineLearningMastery.com. [online] MachineLearningMastery.com. Available at: https://machinelearningmastery.com/expectation-maximization-em-algorithm/ [Accessed 20 Feb. 2024].\n",
    "\n",
    "- Kapadia, S. (2019). Language Models: N-Gram - Towards Data Science. [online] Medium. Available at: https://towardsdatascience.com/introduction-to-language-models-n-gram-e323081503d9 [Accessed 20 Feb. 2024].\n",
    "\n",
    "- freeCodeCamp.org (2018). A history of machine translation from the Cold War to deep learning. [online] freeCodeCamp.org. Available at: https://www.freecodecamp.org/news/a-history-of-machine-translation-from-the-cold-war-to-deep-learning-f1d335ce8b5/ [Accessed 20 Feb. 2024].\n",
    "\n",
    "- Srinidhi, S. (2019). Understanding Word N-grams and N-gram Probability in Natural Language Processing. [online] Medium. Available at: https://towardsdatascience.com/understanding-word-n-grams-and-n-gram-probability-in-natural-language-processing-9d9eef0fa058 [Accessed 20 Feb. 2024]."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c902e53e01be6b5",
   "metadata": {
    "id": "3c902e53e01be6b5"
   },
   "source": [
    "## Self Check\n",
    "***\n",
    "- Have you answered all questions to the best of your ability?\n",
    "Yes, we have.\n",
    "- Is all the required information on the front page, is the file name correct etc.?\n",
    "Indeed, all the required information on the front page has been included.\n",
    "- Anything else you can easily check? (details, terminology, arguments, clearly stated answers etc.?)\n",
    "We have checked, and everything looks good."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
