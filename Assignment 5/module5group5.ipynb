{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0e7365d49bc45f1",
   "metadata": {
    "id": "b0e7365d49bc45f1"
   },
   "source": [
    "# Group 5 - Module 5: Natural Language Processing\n",
    "\n",
    "***\n",
    "### Group Members:\n",
    "* **Nils Dunlop, 20010127-2359, Applied Data Science, e-mail: gusdunlni@student.gu.se (16 hours)**\n",
    "* **Francisco Erazo, 19930613-9214, Applied Data Science, e-mail: guserafr@student.gu.se (16 hours)**\n",
    "\n",
    "#### **We hereby declare that we have both actively participated in solving every exercise. All solutions are entirely our own work, without having taken part of other solutions.\" (This is independent and additional to any declaration that you may encounter in the electronic submission system.)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a40ce4f1465244c7",
   "metadata": {
    "id": "a40ce4f1465244c7"
   },
   "source": [
    "# Assignment 5\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "426429e48a4209ce",
   "metadata": {
    "id": "426429e48a4209ce"
   },
   "source": [
    "## Problem 1: Reading and Reflection\n",
    "***\n",
    "### **(A) AI Problems with a Wide Range of Approaches:**\n",
    "\n",
    "Several AI problems have seen a wide range of approaches, reflecting the diverse computational techniques utilized over the decades. Notable examples include:\n",
    "\n",
    "- **Speech Recognition:** This field has evolved significantly from early phonetic-based systems to modern deep learning approaches. Initially, systems relied on simple pattern matching and rule-based systems to understand spoken language. Over time, statistical models, especially Hidden Markov Models (HMMs), played a crucial role. Currently, deep neural networks, particularly recurrent neural networks (RNNs) and convolutional neural networks (CNNs) dominate the field.\n",
    "   \n",
    "- **Computer Vision:** The evolution of techniques from simple edge detection algorithms and feature extraction methods to sophisticated deep learning models is noteworthy. Techniques have ranged from geometric model fitting and template matching to the use of deep convolutional neural networks (CNNs) for tasks such as image classification, object detection and semantic segmentation.\n",
    "   \n",
    "- **Game Playing:** The development of AI for game playing, from chess and checkers to complex video games, has evolved from brute-force search algorithms and heuristic-based approaches to the use of machine learning techniques, including reinforcement learning and deep learning for strategy optimization.\n",
    "\n",
    "### **(B) Similarities Between Rule-based Translation Systems and Neural Systems:**\n",
    "\n",
    "Despite their differences, rule-based translation systems and state-of-the-art neural systems share notable similarities:\n",
    "\n",
    "- **Structured Knowledge:** Rule-based systems are explicitly programmed with linguistic rules and dictionaries, while neural systems, especially those employing attention mechanisms, implicitly learn structured knowledge about languages through training on large datasets.\n",
    "\n",
    "- **Context Handling:** Both systems aim to handle context in translation, however, in largely different ways. Rule-based systems may use context rules to choose the correct translation based on surrounding words. Neural systems, particularly those using attention mechanisms, can consider broader contexts in a more fluid and dynamic manner.\n",
    "   \n",
    "- **Error Correction:** Both systems have mechanisms for error correction, with neural systems doing this implicitly. Rule-based systems may use post-processing rules to correct common mistakes, while neural systems learn error patterns and corrections during training.\n",
    "\n",
    "### **(C) Situations Favoring Rule-based Solutions Over Modern Approaches:**\n",
    "\n",
    "There are scenarios where a rule-based solution might be preferable to a modern neural or statistical approach:\n",
    "\n",
    "- **Limited Data Scenarios:** For languages or specialized domains where there is a scarcity of training data, rule-based systems can be more practical and effective because they do not require large datasets to perform reasonably well.\n",
    "   \n",
    "- **Predictability and Transparency:** In situations where predictability, transparency, and the ability to audit or explain decisions are crucial (e.g., in some legal or regulatory contexts), rule-based systems offer clear advantages. Their decisions are based on explicit rules that can be examined, understood and modified by humans.\n",
    "\n",
    "- **Real-time Constraints:** When computational resources are limited or real-time performance is essential, the simpler and less resource-intensive rule-based systems can have an advantage over more complex neural models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1b6dbabbbd02376",
   "metadata": {
    "id": "b1b6dbabbbd02376"
   },
   "source": [
    "## Problem 2: Implementation\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2344500c97686b5",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### (A) Warmup: Word Frequencies\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "817613a5b9b4d9cb",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-20T19:34:24.131865100Z",
     "start_time": "2024-02-20T19:34:23.390320500Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              German  \\\n",
      "0  ich erkläre die am freitag , dem 17. dezember ...   \n",
      "1  wie sie feststellen konnten , ist der gefürcht...   \n",
      "2  im parlament besteht der wunsch nach einer aus...   \n",
      "3  heute möchte ich sie bitten - das ist auch der...   \n",
      "4  wie sie sicher aus der presse und dem fernsehe...   \n",
      "\n",
      "                                          English_DE  \n",
      "0  i declare resumed the session of the european ...  \n",
      "1  although , as you will have seen , the dreaded...  \n",
      "2  you have requested a debate on this subject in...  \n",
      "3  in the meantime , i should like to observe a m...  \n",
      "4  you will be aware from the press and televisio...  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import re\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "# Load datasets into DataFrames\n",
    "# The .squeeze() method is used to convert the single-column DataFrames into Series\n",
    "de_en_df = pd.DataFrame({\n",
    "    'German': pd.read_csv(\"dat410_europarl/europarl-v7.de-en.lc.de\", sep='\\n', header=None, names=['text']).squeeze(),\n",
    "    'English_DE': pd.read_csv(\"dat410_europarl/europarl-v7.de-en.lc.en\", sep='\\n', header=None, names=['text']).squeeze()\n",
    "})\n",
    "\n",
    "fr_en_df = pd.DataFrame({\n",
    "    'French': pd.read_csv(\"dat410_europarl/europarl-v7.fr-en.lc.fr\", sep='\\n', header=None, names=['text']).squeeze(),\n",
    "    'English_FR': pd.read_csv(\"dat410_europarl/europarl-v7.fr-en.lc.en\", sep='\\n', header=None, names=['text']).squeeze()\n",
    "})\n",
    "\n",
    "sv_en_df = pd.DataFrame({\n",
    "    'Swedish': pd.read_csv(\"dat410_europarl/europarl-v7.sv-en.lc.sv\", sep='\\n', header=None, names=['text']).squeeze(),\n",
    "    'English_SV': pd.read_csv(\"dat410_europarl/europarl-v7.sv-en.lc.en\", sep='\\n', header=None, names=['text']).squeeze()\n",
    "})\n",
    "\n",
    "# Display the first few rows of the German-English dataset\n",
    "print(de_en_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "843e900b35ca21fe",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-20T19:34:24.200895200Z",
     "start_time": "2024-02-20T19:34:24.131865100Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              French  \\\n",
      "0  je déclare reprise la session du parlement eur...   \n",
      "1  comme vous avez pu le constater , le grand &qu...   \n",
      "2  vous avez souhaité un débat à ce sujet dans le...   \n",
      "3  en attendant , je souhaiterais , comme un cert...   \n",
      "4  je vous invite à vous lever pour cette minute ...   \n",
      "\n",
      "                                          English_FR  \n",
      "0  i declare resumed the session of the european ...  \n",
      "1  although , as you will have seen , the dreaded...  \n",
      "2  you have requested a debate on this subject in...  \n",
      "3  in the meantime , i should like to observe a m...  \n",
      "4  please rise , then , for this minute &apos; s ...  \n"
     ]
    }
   ],
   "source": [
    "# Display the first few rows of the French-English dataset\n",
    "print(fr_en_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d02f8867e40f6f4",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-20T19:34:24.203866500Z",
     "start_time": "2024-02-20T19:34:24.145893800Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             Swedish  \\\n",
      "0  jag förklarar europaparlamentets session återu...   \n",
      "1  som ni kunnat konstatera ägde &quot; den stora...   \n",
      "2  ni har begärt en debatt i ämnet under sammantr...   \n",
      "3  till dess vill jag att vi , som ett antal koll...   \n",
      "4             jag ber er resa er för en tyst minut .   \n",
      "\n",
      "                                          English_SV  \n",
      "0  i declare resumed the session of the european ...  \n",
      "1  although , as you will have seen , the dreaded...  \n",
      "2  you have requested a debate on this subject in...  \n",
      "3  in the meantime , i should like to observe a m...  \n",
      "4  please rise , then , for this minute &apos; s ...  \n"
     ]
    }
   ],
   "source": [
    "# Display the first few rows of the Swedish-English dataset\n",
    "print(sv_en_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3c9bd61df230e8d2",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-20T19:34:24.258872900Z",
     "start_time": "2024-02-20T19:34:24.163880400Z"
    }
   },
   "outputs": [],
   "source": [
    "# Function to calculate word frequencies\n",
    "def calculate_word_frequencies(df_column):\n",
    "    word_counts = Counter()\n",
    "    for text in df_column:\n",
    "        # Remove punctuation and other non-word characters\n",
    "        words = re.findall(r'\\b\\w+\\b', text.lower())\n",
    "        word_counts.update(words)\n",
    "    return word_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "adcf00330048fa75",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-20T19:34:25.002292Z",
     "start_time": "2024-02-20T19:34:24.204866600Z"
    }
   },
   "outputs": [],
   "source": [
    "# Calculate word frequencies for each language\n",
    "de_word_counts = calculate_word_frequencies(de_en_df['German'])\n",
    "en_de_word_counts = calculate_word_frequencies(de_en_df['English_DE'])\n",
    "\n",
    "fr_word_counts = calculate_word_frequencies(fr_en_df['French'])\n",
    "en_fr_word_counts = calculate_word_frequencies(fr_en_df['English_FR'])\n",
    "\n",
    "sv_word_counts = calculate_word_frequencies(sv_en_df['Swedish'])\n",
    "en_sv_word_counts = calculate_word_frequencies(sv_en_df['English_SV'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c66efd3074499963",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-20T19:34:25.024287700Z",
     "start_time": "2024-02-20T19:34:25.007289700Z"
    }
   },
   "outputs": [],
   "source": [
    "# Function to print most common words\n",
    "def print_most_common(word_counts, language, num=10):\n",
    "    print(f\"Most common words in {language}:\")\n",
    "    for word, count in word_counts.most_common(num):\n",
    "        print(f\"{word}: {count}\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1c678acf838a13c",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-20T19:34:25.051277500Z",
     "start_time": "2024-02-20T19:34:25.020291200Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most common words in German:\n",
      "die: 10521\n",
      "der: 9374\n",
      "und: 7028\n",
      "in: 4175\n",
      "zu: 3169\n",
      "den: 2976\n",
      "wir: 2863\n",
      "daß: 2738\n",
      "ich: 2670\n",
      "das: 2669\n",
      "\n",
      "\n",
      "Most common words in English (DE):\n",
      "the: 19853\n",
      "of: 9633\n",
      "to: 9069\n",
      "and: 7307\n",
      "in: 6278\n",
      "is: 4478\n",
      "that: 4441\n",
      "a: 4438\n",
      "we: 3372\n",
      "this: 3362\n",
      "\n",
      "\n",
      "Most common words in French:\n",
      "apos: 16729\n",
      "de: 14528\n",
      "la: 9746\n",
      "et: 6620\n",
      "l: 6536\n",
      "le: 6177\n",
      "à: 5588\n",
      "les: 5587\n",
      "des: 5232\n",
      "que: 4797\n",
      "\n",
      "\n",
      "Most common words in English (FR):\n",
      "the: 19627\n",
      "of: 9534\n",
      "to: 8992\n",
      "and: 7214\n",
      "in: 6197\n",
      "is: 4453\n",
      "that: 4421\n",
      "a: 4388\n",
      "we: 3341\n",
      "this: 3332\n",
      "\n",
      "\n",
      "Most common words in Swedish:\n",
      "att: 9181\n",
      "och: 7038\n",
      "i: 5954\n",
      "det: 5687\n",
      "som: 5028\n",
      "för: 4959\n",
      "av: 4013\n",
      "är: 3840\n",
      "en: 3724\n",
      "vi: 3211\n",
      "\n",
      "\n",
      "Most common words in English (SV):\n",
      "the: 19327\n",
      "of: 9344\n",
      "to: 8814\n",
      "and: 6949\n",
      "in: 6124\n",
      "is: 4400\n",
      "that: 4357\n",
      "a: 4271\n",
      "we: 3223\n",
      "this: 3222\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print the 10 most frequent words for each language\n",
    "print_most_common(de_word_counts, \"German\")\n",
    "print_most_common(en_de_word_counts, \"English (DE)\")\n",
    "\n",
    "print_most_common(fr_word_counts, \"French\")\n",
    "print_most_common(en_fr_word_counts, \"English (FR)\")\n",
    "\n",
    "print_most_common(sv_word_counts, \"Swedish\")\n",
    "print_most_common(en_sv_word_counts, \"English (SV)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2616bdd5e8d0c0ba",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-20T19:34:25.068289700Z",
     "start_time": "2024-02-20T19:34:25.052288800Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability of 'speaker' in German: 0.000000\n",
      "Probability of 'zebra' in German: 0.000000\n",
      "Probability of 'speaker' in English (DE): 0.000042\n",
      "Probability of 'zebra' in English (DE): 0.000000\n",
      "Probability of 'speaker' in French: 0.000000\n",
      "Probability of 'zebra' in French: 0.000000\n",
      "Probability of 'speaker' in English (FR): 0.000046\n",
      "Probability of 'zebra' in English (FR): 0.000000\n",
      "Probability of 'speaker' in Swedish: 0.000000\n",
      "Probability of 'zebra' in Swedish: 0.000000\n",
      "Probability of 'speaker' in English (SV): 0.000039\n",
      "Probability of 'zebra' in English (SV): 0.000000\n"
     ]
    }
   ],
   "source": [
    "def print_probabilities(word_counts, language, total_words):\n",
    "    for word in ['speaker', 'zebra']:\n",
    "        if word in word_counts:\n",
    "            prob = word_counts[word] / total_words\n",
    "        else:\n",
    "            prob = 0\n",
    "        print(f\"Probability of '{word}' in {language}: {prob:.6f}\")\n",
    "\n",
    "total_de_words = sum(de_word_counts.values())\n",
    "total_en_de_words = sum(en_de_word_counts.values())\n",
    "total_fr_words = sum(fr_word_counts.values())\n",
    "total_en_fr_words = sum(en_fr_word_counts.values())\n",
    "total_sv_words = sum(sv_word_counts.values())\n",
    "total_en_sv_words = sum(en_sv_word_counts.values())\n",
    "\n",
    "print_probabilities(de_word_counts, \"German\", total_de_words)\n",
    "print_probabilities(en_de_word_counts, \"English (DE)\", total_en_de_words)\n",
    "print_probabilities(fr_word_counts, \"French\", total_fr_words)\n",
    "print_probabilities(en_fr_word_counts, \"English (FR)\", total_en_fr_words)\n",
    "print_probabilities(sv_word_counts, \"Swedish\", total_sv_words)\n",
    "print_probabilities(en_sv_word_counts, \"English (SV)\", total_en_sv_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f342dac",
   "metadata": {},
   "source": [
    "The output indicates that the word 'speaker' is found in the English versions of the texts, specifically English (DE), English (FR), and English (SV), with respective probabilities of 0.000042, 0.000046, and 0.000039. This suggests a limited presence of 'speaker' only within these English translations. In contrast, the absence of 'speaker' in German, French, and Swedish, as shown by a 0 probability, confirms its unavailability in these languages.\n",
    "\n",
    "Regarding the word 'zebra', its 0 probability across all examined languages implies that it does not feature in any of the texts. This reveals that 'speaker' is present solely in the English corpora, while 'zebra' is not found in any of the languages analyzed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "603802a163b48ed7",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### (B) Language Modeling\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b3af9522c34a79e7",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-20T19:34:25.800280200Z",
     "start_time": "2024-02-20T19:34:25.073279700Z"
    }
   },
   "outputs": [],
   "source": [
    "# Tokenize the text and add start and end tokens\n",
    "# The words between the start and end markers are determined and tokenized\n",
    "tokenized_text_de = [('<START>',) + tuple(re.findall(r'\\b\\w+\\b', sentence.lower())) + ('<END>',) for sentence in de_en_df['English_DE']]\n",
    "tokenized_text_fr = [('<START>',) + tuple(re.findall(r'\\b\\w+\\b', sentence.lower())) + ('<END>',) for sentence in fr_en_df['English_FR']]\n",
    "tokenized_text_sv = [('<START>',) + tuple(re.findall(r'\\b\\w+\\b', sentence.lower())) + ('<END>',) for sentence in sv_en_df['English_SV']]\n",
    "\n",
    "# Count individual words and bigrams\n",
    "word_counts_de = Counter()\n",
    "bigram_counts_de = Counter()\n",
    "word_counts_fr = Counter()\n",
    "bigram_counts_fr = Counter()\n",
    "word_counts_sv = Counter()\n",
    "bigram_counts_sv = Counter()\n",
    "\n",
    "# Update word counts and form bigram tuples of the words\n",
    "for sentence in tokenized_text_de:\n",
    "    word_counts_de.update(sentence)\n",
    "    bigram_counts_de.update(zip(sentence[:-1], sentence[1:]))\n",
    "\n",
    "for sentence in tokenized_text_fr:\n",
    "    word_counts_fr.update(sentence)\n",
    "    bigram_counts_fr.update(zip(sentence[:-1], sentence[1:]))\n",
    "\n",
    "for sentence in tokenized_text_sv:\n",
    "    word_counts_sv.update(sentence)\n",
    "    bigram_counts_sv.update(zip(sentence[:-1], sentence[1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a0e27540712378b5",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-20T19:34:25.860067500Z",
     "start_time": "2024-02-20T19:34:25.813062800Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability of the sentence 'The economic impact of the legislation was significant.' in German-English data: 5.452363580933155e-28\n",
      "Probability of the sentence 'Diplomatic efforts were intensified to resolve the conflict.' in French-English data: 5.274458597849841e-34\n",
      "Probability of the sentence 'Environmental policies have become increasingly important.' in Swedish-English data: 1.381206394015978e-26\n",
      "Probability of the sentence with an OOV word 'this is a quixotic test sentence' in German-English data: 1.0607721443308697e-21\n",
      "Probability of the sentence with an OOV word 'this is a quixotic test sentence' in French-English data: 1.0477026424612885e-21\n",
      "Probability of the sentence with an OOV word 'this is a quixotic test sentence' in Swedish-English data: 1.0820221311118906e-21\n"
     ]
    }
   ],
   "source": [
    "# Compute the probability of a sentence using a bigram model with Laplace smoothing\n",
    "def bigram_sentence_probability(sentence, word_counts, bigram_counts, total_words, smoothing=1):\n",
    "    sentence = ('<START>',) + tuple(re.findall(r'\\b\\w+\\b', sentence.lower())) + ('<END>',)\n",
    "    bigram_probs = []\n",
    "\n",
    "    for first_word, second_word in zip(sentence[:-1], sentence[1:]):\n",
    "        bigram_count = bigram_counts[(first_word, second_word)]\n",
    "        word_count = word_counts[first_word]\n",
    "        \n",
    "        # Apply Laplace smoothing\n",
    "        prob = (bigram_count + smoothing) / (word_count + smoothing * (total_words + 1))\n",
    "        bigram_probs.append(prob)\n",
    "\n",
    "    # Logarithms is used to avoid underflow with long sentences\n",
    "    log_probs = np.log(bigram_probs)\n",
    "    log_prob_sentence = np.sum(log_probs)\n",
    "\n",
    "    return np.exp(log_prob_sentence)\n",
    "\n",
    "# Test the bigram_sentence_probability for the German-English dataset\n",
    "de_sentence = \"The economic impact of the legislation was significant.\" # Arbitrary parliament sentence\n",
    "de_prob = bigram_sentence_probability(de_sentence, word_counts_de, bigram_counts_de, len(word_counts_de))\n",
    "print(f\"Probability of the sentence '{de_sentence}' in German-English data: {de_prob}\")\n",
    "\n",
    "# Test the bigram_sentence_probability for the French-English dataset\n",
    "fr_sentence = \"Diplomatic efforts were intensified to resolve the conflict.\" # Arbitrary parliament sentence\n",
    "fr_prob = bigram_sentence_probability(fr_sentence, word_counts_fr, bigram_counts_fr, len(word_counts_fr))\n",
    "print(f\"Probability of the sentence '{fr_sentence}' in French-English data: {fr_prob}\")\n",
    "\n",
    "# Test the bigram_sentence_probability for the Swedish-English dataset\n",
    "sv_sentence = \"Environmental policies have become increasingly important.\" # Arbitrary parliament sentence\n",
    "sv_prob = bigram_sentence_probability(sv_sentence, word_counts_sv, bigram_counts_sv, len(word_counts_sv))\n",
    "print(f\"Probability of the sentence '{sv_sentence}' in Swedish-English data: {sv_prob}\")\n",
    "\n",
    "# Out of Vocabulary Word Test\n",
    "oov_sentence = \"this is a quixotic test sentence\"\n",
    "oov_prob_de = bigram_sentence_probability(oov_sentence, word_counts_de, bigram_counts_de, len(word_counts_de))\n",
    "oov_prob_fr = bigram_sentence_probability(oov_sentence, word_counts_fr, bigram_counts_fr, len(word_counts_fr))\n",
    "oov_prob_sv = bigram_sentence_probability(oov_sentence, word_counts_sv, bigram_counts_sv, len(word_counts_sv))\n",
    "print(f\"Probability of the sentence with an OOV word '{oov_sentence}' in German-English data: {oov_prob_de}\")\n",
    "print(f\"Probability of the sentence with an OOV word '{oov_sentence}' in French-English data: {oov_prob_fr}\")\n",
    "print(f\"Probability of the sentence with an OOV word '{oov_sentence}' in Swedish-English data: {oov_prob_sv}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Laplace smoothing is used in the cases in which we compute the probability of a sentence that contains a word that is not in the texts, in this case 'quixotic'. Laplace smoothing ensures that every possible word and word pair has a non-zero probability, even if it hasn't been seen in the training data. This means that the word 'quixotic' is assigned with a small, non-zero probability, which helps prevent the model from assigning a zero probability to sentences with words that were not present in the training corpus.\n",
    "\n",
    "Something, that we noticed in our test before is that the lenght of the sentences affect the probability because of bigram frenquency combinations. So, for very long sentences, it is possible that the probability is very low even if the sentence is in the language. We tested with a sentence of 100 words and the probability was 0. "
   ],
   "id": "c0bd9310251a5afb"
  },
  {
   "cell_type": "markdown",
   "id": "ce29fab969b8a126",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### (C) Translation Model\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "75ae3e47078f3f1",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-20T19:34:25.889067500Z",
     "start_time": "2024-02-20T19:34:25.837067200Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_parallel_sentences(df_en, df_foreign):\n",
    "    return list(zip(df_en, df_foreign))\n",
    "\n",
    "# Create parallel corpora\n",
    "de_parallel_sentences = create_parallel_sentences(de_en_df['English_DE'], de_en_df['German'])\n",
    "fr_parallel_sentences = create_parallel_sentences(fr_en_df['English_FR'], fr_en_df['French'])\n",
    "sv_parallel_sentences = create_parallel_sentences(sv_en_df['English_SV'], sv_en_df['Swedish'])\n",
    "\n",
    "# Select which language pair to use for the IBM Model 1\n",
    "# We decided to look into English and German parallel sentences\n",
    "parallel_sentences = de_parallel_sentences\n",
    "\n",
    "# Initialize translation probabilities uniformly\n",
    "def initialize_translation_probabilities(parallel_sentences):\n",
    "    all_en_words = set()\n",
    "    all_foreign_words = set()\n",
    "\n",
    "    # Collect English and foreign words from the parallel sentences\n",
    "    for en, foreign in parallel_sentences:\n",
    "        en_words = re.findall(r'\\b\\w+\\b', en.lower())\n",
    "        foreign_words = re.findall(r'\\b\\w+\\b', foreign.lower())\n",
    "        all_en_words.update(en_words)\n",
    "        all_foreign_words.update(foreign_words)\n",
    "\n",
    "    # Add a buffer for the <NULL> token\n",
    "    all_en_words.add('<NULL>')\n",
    "\n",
    "    # Calculate the initial probability\n",
    "    initial_prob = 1 / len(all_en_words)\n",
    "\n",
    "    # Create the translation probabilities dictionary\n",
    "    translation_probs = {}\n",
    "    for f_word in all_foreign_words:\n",
    "        translation_probs[f_word] = {}\n",
    "        for e_word in all_en_words:\n",
    "            translation_probs[f_word][e_word] = initial_prob\n",
    "\n",
    "    return translation_probs"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def find_top_translations(translation_probs, english_word, top_n=10):\n",
    "    likely_translations = []\n",
    "    for f_word, e_words_probs in translation_probs.items():\n",
    "        if english_word in e_words_probs:\n",
    "            likely_translations.append((f_word, e_words_probs[english_word]))\n",
    "    likely_translations.sort(key=lambda x: x[1], reverse=True)\n",
    "    return likely_translations[:top_n]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-20T19:34:25.891067400Z",
     "start_time": "2024-02-20T19:34:25.863080400Z"
    }
   },
   "id": "9ba24aa07c9ba740",
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8d3f931b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-20T19:34:25.917067600Z",
     "start_time": "2024-02-20T19:34:25.886072800Z"
    }
   },
   "outputs": [],
   "source": [
    "def em_algorithm(parallel_sentences, num_iterations=5, target_word=\"european\"):\n",
    "    print(\"Initializing translation probabilities...\")\n",
    "    translation_probs = initialize_translation_probabilities(parallel_sentences)\n",
    "\n",
    "    for i in range(num_iterations):\n",
    "        print(f\"Starting iteration {i+1}/{num_iterations}...\")\n",
    "\n",
    "        count_e_f = defaultdict(lambda: defaultdict(float))\n",
    "        total_e = defaultdict(float)\n",
    "\n",
    "        # E-step\n",
    "        for en_sentence, foreign_sentence in parallel_sentences:\n",
    "            en_words = ['<NULL>'] + re.findall(r'\\b\\w+\\b', en_sentence.lower())\n",
    "            foreign_words = re.findall(r'\\b\\w+\\b', foreign_sentence.lower())\n",
    "            for f_word in foreign_words:\n",
    "                denom_c = 0\n",
    "                for e_word in en_words:\n",
    "                    denom_c += translation_probs[f_word][e_word]\n",
    "                for e_word in en_words:\n",
    "                    delta = translation_probs[f_word][e_word] / denom_c\n",
    "                    count_e_f[f_word][e_word] += delta\n",
    "                    total_e[e_word] += delta\n",
    "\n",
    "        # M-step\n",
    "        for f_word, e_words_probs in translation_probs.items():\n",
    "            for e_word in e_words_probs:\n",
    "                e_word_total = total_e[e_word]\n",
    "                if e_word_total > 0:\n",
    "                    translation_probs[f_word][e_word] = count_e_f[f_word][e_word] / e_word_total\n",
    "\n",
    "        # After each iteration, find and print top translations for the target word\n",
    "        top_translations = find_top_translations(translation_probs, target_word)\n",
    "        print(f\"\\nTop 10 translations for '{target_word}' after iteration {i+1}:\")\n",
    "        for foreign_word, prob in top_translations:\n",
    "            print(f\"{foreign_word}: {prob}\")\n",
    "        print()\n",
    "\n",
    "    return translation_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4c8584d6227de328",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-20T20:11:10.548812700Z",
     "start_time": "2024-02-20T19:34:26.085068100Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting the EM algorithm...\n",
      "Initializing translation probabilities...\n",
      "Starting iteration 1/5...\n",
      "\n",
      "Top 10 translations for 'european' after iteration 1:\n",
      "der: 0.04924442682981817\n",
      "die: 0.046802621154413596\n",
      "und: 0.028525924918692182\n",
      "europäischen: 0.02447797001659754\n",
      "in: 0.01868630204831187\n",
      "den: 0.013677785227281396\n",
      "union: 0.013304013680909268\n",
      "zu: 0.012170667079711855\n",
      "europäische: 0.011874041186260318\n",
      "das: 0.011722937742977274\n",
      "\n",
      "Starting iteration 2/5...\n",
      "\n",
      "Top 10 translations for 'european' after iteration 2:\n",
      "europäischen: 0.15227090361878062\n",
      "der: 0.0884782136797811\n",
      "die: 0.0743837520462704\n",
      "europäische: 0.0649963326964511\n",
      "union: 0.04779552637759021\n",
      "und: 0.04100530475565621\n",
      "in: 0.028724906323091815\n",
      "den: 0.020814212714958237\n",
      "das: 0.0167381101250379\n",
      "zu: 0.01586282793135845\n",
      "\n",
      "Starting iteration 3/5...\n",
      "\n",
      "Top 10 translations for 'european' after iteration 3:\n",
      "europäischen: 0.34844350160392445\n",
      "europäische: 0.146996618913835\n",
      "der: 0.07753193971174076\n",
      "die: 0.059046929124181906\n",
      "union: 0.049096988507153994\n",
      "und: 0.0293777950645934\n",
      "in: 0.022671697001550178\n",
      "den: 0.017049370606169983\n",
      "das: 0.012885488221195607\n",
      "für: 0.011848600698311503\n",
      "\n",
      "Starting iteration 4/5...\n",
      "\n",
      "Top 10 translations for 'european' after iteration 4:\n",
      "europäischen: 0.48612230571788584\n",
      "europäische: 0.20916498383045343\n",
      "der: 0.05784884643372422\n",
      "die: 0.040285112476897936\n",
      "union: 0.03281786631771734\n",
      "und: 0.017673852071668824\n",
      "in: 0.015225639753741253\n",
      "den: 0.012015599997872297\n",
      "das: 0.008627005386665098\n",
      "für: 0.007468409345575258\n",
      "\n",
      "Starting iteration 5/5...\n",
      "\n",
      "Top 10 translations for 'european' after iteration 5:\n",
      "europäischen: 0.5632761859565995\n",
      "europäische: 0.2475383085188334\n",
      "der: 0.042232841286649406\n",
      "die: 0.027021545239374084\n",
      "union: 0.020943386322625646\n",
      "und: 0.01038648858589231\n",
      "in: 0.009981265254037574\n",
      "den: 0.00832870844471093\n",
      "das: 0.005713162414687346\n",
      "für: 0.004639426042403674\n",
      "\n",
      "EM algorithm completed.\n"
     ]
    }
   ],
   "source": [
    "# Before running the EM algorithm\n",
    "print(\"Starting the EM algorithm...\")\n",
    "translation_probs = em_algorithm(parallel_sentences)\n",
    "print(\"EM algorithm completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe9ac3ca",
   "metadata": {},
   "source": [
    "Over the iterations, the IBM Model 1 increasingly identifies \"europäischen\" and \"europäische\" as top translations for \"european,\" indicating improved learning of semantic relevance. Initially, common words like \"der\" and \"die\" appear prominently due to their frequency but later iterations better reflect the actual meaning, despite some residual noise from high-frequency and less relevant words. The model's ability to learn the correct translations for \"european\" demonstrates the effectiveness of the EM algorithm in refining translation probabilities over multiple iterations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1843b1f7d46af262",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Self-check: if our goal is to translate from some language into English, why does our conditional probability seem to be written backwards? Why don't we estimate P(e|f) instead?\n",
    "The conditional probability in translation models may appear counterintuitive at first glance. In IBM Model 1, we calculate `P(f|e)` rather than `P(e|f)` because we are establishing an alignment model rather than a direct translation model. This model determines the probability that a word in a foreign language aligns with or translates from an English word. In short we are modeling the generative process of the foreign language given the English text. This approach allows the model to be symmetric and supporting translation in either direction which is beneficial for creating a bilingual model that can assist in translating both to and from English."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1675b37c1c805994",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### (D) Decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "579e8c048b451fa4",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-20T20:11:10.630824800Z",
     "start_time": "2024-02-20T20:11:10.549839400Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translated sentence: reasonably house\n",
      "Translated sentence: i speaking colourful bisschen deutsch\n"
     ]
    }
   ],
   "source": [
    "def greedy_decoder(foreign_sentence, translation_probs):\n",
    "    # Tokenize the foreign sentence\n",
    "    # The regular expression \\b\\w+\\b matches whole words\n",
    "    foreign_words = re.findall(r'\\b\\w+\\b', foreign_sentence.lower())\n",
    "    translated_sentence = []\n",
    "\n",
    "    # For each foreign word find the English word with the highest translation probability\n",
    "    for f_word in foreign_words:\n",
    "        best_prob = 0\n",
    "        best_english_word = None\n",
    "        for e_word, prob in translation_probs.get(f_word, {}).items():\n",
    "            if prob > best_prob:\n",
    "                best_prob = prob\n",
    "                best_english_word = e_word\n",
    "\n",
    "        # If no translation is found use the foreign word\n",
    "        if best_english_word:\n",
    "            translated_sentence.append(best_english_word)\n",
    "        else:\n",
    "            translated_sentence.append(f_word)\n",
    "\n",
    "    return ' '.join(translated_sentence)\n",
    "\n",
    "foreign_sentence_1 = \"das Haus\" # The house\n",
    "foreign_sentence_2 = \"Ich spreche ein bisschen Deutsch.\" # I speak a little German.\n",
    "english_translation_1 = greedy_decoder(foreign_sentence_1, translation_probs)\n",
    "english_translation_2 = greedy_decoder(foreign_sentence_2, translation_probs)\n",
    "print(f\"Translated sentence: {english_translation_1}\")\n",
    "print(f\"Translated sentence: {english_translation_2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f7227e06157f39",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Simplifying Assumptions:\n",
    "- **Word-by-Word Translation**: This approach assumes that a sentence can be translated word-by-word which rarely is the case given differences in grammar and sentence structure between languages.\n",
    "- **Ignoring Word Order**: The resulting translation does not consider the correct word order in English.\n",
    "- **No Context Consideration**: Each word is translated independently of its context which can lead to incorrect translations for words with multiple meanings.\n",
    "- **Out-of-Vocabulary Words**: Words not seen in the training data will not be translated correctly.\n",
    "\n",
    "When analyzing the output, it is clear that the method used for translation is quite rudimentary and that the results are imperfect. These translations serve as a starting point and highlight areas where more sophisticated models and techniques are necessary for accurate translation.\n",
    "\n",
    "Finding the English sentence that has the highest probability given a source-language sentence is algorithmically challenging due to the vast number of possible translations and the complexity of the translation model. \n",
    "- The search space for the most probable English sentence is enormous and the model must consider all possible translations and alignments to find the most probable sentence. This process is computationally intensive and requires sophisticated algorithms to efficiently search the space of possible translations. \n",
    "- The probability P(E∣F) depends on both the local context (e.g., word order, grammar) and the broader context (e.g., the sentence's topic or style). Accurately modeling these dependencies requires complex probabilistic models.\n",
    "- Evaluating P(E∣F) for every possible translation E is computationally intensive, especially for long sentences or large vocabularies.\n",
    "- Words can have multiple meanings depending on the context, making it difficult to select the correct translation without understanding the entire sentence.\n",
    "- Phrases that do not translate directly between languages pose a particular challenge, as they require understanding beyond the word level."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abc48290c4db11b3",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Problem 3: Discussion\n",
    "***\n",
    "\n",
    "#### (A) Propose a number of different evaluation protocols for machine translation systems and discuss their advantages and disadvantages. What does it mean for a translation to be \"good\"? Minimally, you should think of one manual and one automatic procedure. (The point here is not that you should search the web but that you should try to come up with your own ideas.)\n",
    "The evaluation of machine translation systems can encompass methods of **Human Evaluation** and **Automatic Evaluation**.\n",
    "\n",
    "**Human Evaluation** might include protocols such as:\n",
    "- **Advantages**: This method provides a nuanced understanding that encompasses assessments of readability, fluency, and the conveyance of original meaning.\n",
    "- **Disadvantages**: However, it is often time-consuming and costly with a susceptibility to human bias.\n",
    "- **Protocols**: \n",
    "  - **Ranking/Scoring**: In this protocol, experts assign scores to translations based on criteria including fluency, adequacy and overall quality.\n",
    "  - **Post-editing Analysis**: This involves an examination of the editing effort necessary to refine machine translations to acceptable standards.\n",
    "\n",
    "**Automatic Evaluation** offers different advantages and follows distinct protocols:\n",
    "- **Advantages**: It is recognized for its speed and cost-effectiveness while providing consistent and repeatable metrics.\n",
    "- **Disadvantages**: The limitation lies in its potential failure to fully grasp semantic correctness and the nuances of fluency.\n",
    "- **Protocols**: \n",
    "  - **BLEU Score**: This widely-used metric compares the n-gram overlap of the machines output with that of a reference translation.\n",
    "  - **TER**: The Translation Edit Rate measures the number of edits required to change a machine-generated translation into an acceptable version based on a reference.\n",
    "\n",
    "Determining a \"good\" translation involves considering accuracy in reflecting the original meaning, maintaining the source's tone, and ensuring fluency within the target language's context. Both human and automatic methods strive to measure these attributes and both have their own degree of efficacy.\n",
    "\n",
    "#### (B) The following example shows a number of sentences automatically translated from Estonian into English. In Estonian, ta means either \"he\" or \"she\", depending on whom we're talking about. Please comment on the translated sentences: what do you think are the technical reasons we see this effect? Do you consider this to be a bug or a feature?\n",
    "The example illustrates that the translation system has chosen \"he\" for professions typically perceived as male-dominated (\"doctor\", \"computer programmer\") and \"she\" for roles often stereotypically female (\"nurse\", babysitter\").\n",
    "\n",
    "This effect likely arises from the training data bias, where the model has learned correlations between gender and profession based on the predominance of these associations in the dataset. While some may view this as a reflection of current societal roles and therefore a feature, it also reinforces gender stereotypes, which can be seen as a bug. As a result, this highlights the need for careful consideration in how translation systems handle gender by potentially providing gender-neutral options or allowing users to specify gender where relevant.\n",
    "\n",
    "#### (c) Below, we consider three sentences that include the English word bat and their automatic translation into Swedish by Google Translate. Why do you think the translation system has been able to select the correct translation of bat in the first two cases? What might be the reason that it has invented a new nonsense word in the third case?\n",
    "When examining the automatic translation of sentences containing the word \"bat\" from English to Swedish via Google Translate we find two successful translations and one anomaly. The first two sentences which reference \"bat\" as an animal are correctly translated due to distinct contextual clues phrases like \"hit the ball\" and \"eats insects\" guide the translation system to the zoological meaning. However, the third sentence presents a peculiar case where the translation system generates a nonsensical term.\n",
    "\n",
    "This anomaly arises because the word \"bat\" can contextually fit both as an animal in the woods and as a piece of sports equipment made of wood. The ambiguity here stems from the translation model's inability to distinguish the term \"bat\" without a clear contextual indicator favoring the sports equipment interpretation. The model defaults to a nonsensical translation which underscores the critical role of context in accurate translation and the challenges faced by models relying solely on word-level data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88753424",
   "metadata": {},
   "source": [
    "## References\n",
    "***\n",
    "\n",
    "- Brownlee, J. (2019). A Gentle Introduction to Expectation-Maximization (EM Algorithm) - MachineLearningMastery.com. [online] MachineLearningMastery.com. Available at: https://machinelearningmastery.com/expectation-maximization-em-algorithm/ [Accessed 20 Feb. 2024].\n",
    "\n",
    "- Kapadia, S. (2019). Language Models: N-Gram - Towards Data Science. [online] Medium. Available at: https://towardsdatascience.com/introduction-to-language-models-n-gram-e323081503d9 [Accessed 20 Feb. 2024].\n",
    "\n",
    "- freeCodeCamp.org (2018). A history of machine translation from the Cold War to deep learning. [online] freeCodeCamp.org. Available at: https://www.freecodecamp.org/news/a-history-of-machine-translation-from-the-cold-war-to-deep-learning-f1d335ce8b5/ [Accessed 20 Feb. 2024].\n",
    "\n",
    "- Srinidhi, S. (2019). Understanding Word N-grams and N-gram Probability in Natural Language Processing. [online] Medium. Available at: https://towardsdatascience.com/understanding-word-n-grams-and-n-gram-probability-in-natural-language-processing-9d9eef0fa058 [Accessed 20 Feb. 2024].\n",
    "\n",
    "- Collins, M. (n.d.). Statistical Machine Translation: IBM Models 1 and 2. [online] Available at: https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1162/handouts/Collins_annotated.pdf [Accessed 20 Feb. 2024]."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c902e53e01be6b5",
   "metadata": {
    "id": "3c902e53e01be6b5"
   },
   "source": [
    "## Self Check\n",
    "***\n",
    "- Have you answered all questions to the best of your ability?\n",
    "Yes, we have.\n",
    "- Is all the required information on the front page, is the file name correct etc.?\n",
    "Indeed, all the required information on the front page has been included.\n",
    "- Anything else you can easily check? (details, terminology, arguments, clearly stated answers etc.?)\n",
    "We have checked, and everything looks good."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
