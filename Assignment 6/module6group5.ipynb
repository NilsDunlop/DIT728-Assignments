{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0e7365d49bc45f1",
   "metadata": {
    "id": "b0e7365d49bc45f1"
   },
   "source": [
    "# Group 5 - Module 6: Game Playing Systems\n",
    "\n",
    "***\n",
    "### Group Members:\n",
    "* **Nils Dunlop, 20010127-2359, Applied Data Science, e-mail: gusdunlni@student.gu.se (16 hours)**\n",
    "* **Francisco Erazo, 19930613-9214, Applied Data Science, e-mail: guserafr@student.gu.se (16 hours)**\n",
    "\n",
    "#### **We hereby declare that we have both actively participated in solving every exercise. All solutions are entirely our own work, without having taken part of other solutions.\" (This is independent and additional to any declaration that you may encounter in the electronic submission system.)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a40ce4f1465244c7",
   "metadata": {
    "id": "a40ce4f1465244c7"
   },
   "source": [
    "# Assignment 6\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "426429e48a4209ce",
   "metadata": {
    "id": "426429e48a4209ce"
   },
   "source": [
    "## Problem 1: Reading and Reflection\n",
    "***\n",
    "AlphaGo is a computer program designed to play the board game Go. It was created by DeepMind Technologies, now a part of Google. Go is known for its complex strategies and the vast number of possible moves, presenting a significant challenge for traditional AI methods. AlphaGo uses advanced deep neural networks and Monte Carlo Tree Search (MCTS), enabling it to learn from a large amount of data, including recorded games between human experts and games it played against itself. Through this learning process, AlphaGo developed judgment and intuition similar to human players, allowing it to accurately predict moves and game outcomes.\n",
    "\n",
    "The architecture of AlphaGo includes policy networks that suggest probable next moves, and value networks that predict the game's winner from any position, marking a step forward in using AI to tackle complex problems. Its training involved supervised learning from games played by human experts and reinforcement learning from self-play. This approach enabled the system to continuously refine its strategies and adjust to new challenges. The integration of MCTS with neural networks enabled efficient exploration and evaluation of possible moves, balancing between relying on known effective strategies and exploring new ones.\n",
    "\n",
    "AlphaGo's success was demonstrated by its 99.8% win rate against other Go programs and its historic victory over European Go champion Fan Hui 5-0, marking the first time a computer program defeated a professional human player in Go. Given Go's complexity compared to chess, this achievement was seen as a significant milestone in AI.\n",
    "\n",
    "The strategies employed by AlphaGo, and its implications, extend beyond just games. They showcase the potential of deep learning and reinforcement learning to address complex issues in various fields.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1b6dbabbbd02376",
   "metadata": {
    "id": "b1b6dbabbbd02376"
   },
   "source": [
    "## Problem 2: Implementation\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5fa74a9d3d66636",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Tic-Tac-Toe Game Board and Rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b0ef4c4cd3915d54",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-26T18:06:16.140621700Z",
     "start_time": "2024-02-26T18:06:16.099741Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "\n",
    "class TicTacToe:\n",
    "    def __init__(self):\n",
    "        # Initialize a 3x3 Tic Tac Toe board\n",
    "        self.board = [[' ' for _ in range(3)] for _ in range(3)]\n",
    "        self.current_turn = 'X'  # Start with player 'X'\n",
    "        self.game_over = False\n",
    "        self.winner = None\n",
    "\n",
    "    def move(self, row, col):\n",
    "        # Place a move on the board if the cell is empty\n",
    "        if self.board[row][col] == ' ':\n",
    "            self.board[row][col] = self.current_turn\n",
    "            self.check_winner()  # Check for a winner after the move\n",
    "            self.current_turn = 'O' if self.current_turn == 'X' else 'X'  # Switch turns\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def check_winner(self):\n",
    "        # Check all win conditions (rows, columns, diagonals)\n",
    "        for i in range(3):\n",
    "            # Check rows and columns\n",
    "            if self.board[i][0] == self.board[i][1] == self.board[i][2] != ' ' or \\\n",
    "                    self.board[0][i] == self.board[1][i] == self.board[2][i] != ' ':\n",
    "                self.game_over = True\n",
    "                self.winner = self.board[i][0]\n",
    "                return\n",
    "        # Check diagonals\n",
    "        if self.board[0][0] == self.board[1][1] == self.board[2][2] != ' ' or \\\n",
    "                self.board[0][2] == self.board[1][1] == self.board[2][0] != ' ':\n",
    "            self.game_over = True\n",
    "            self.winner = self.board[1][1]\n",
    "            return\n",
    "        # Check for a draw (no empty spaces left)\n",
    "        if all(self.board[row][col] != ' ' for row in range(3) for col in range(3)):\n",
    "            self.game_over = True\n",
    "\n",
    "    def get_legal_moves(self):\n",
    "        # Return a list of all empty spaces on the board\n",
    "        return [(row, col) for row in range(3) for col in range(3) if self.board[row][col] == ' ']\n",
    "\n",
    "    def clone(self):\n",
    "        # Create a copy of the game state\n",
    "        clone = TicTacToe()\n",
    "        clone.board = [row[:] for row in self.board]\n",
    "        clone.current_turn = self.current_turn\n",
    "        clone.game_over = self.game_over\n",
    "        clone.winner = self.winner\n",
    "        return clone\n",
    "\n",
    "    def print_board(self, generation=None):\n",
    "        # Print the current state of the board\n",
    "        if generation is not None:\n",
    "            print(f\"Board State at Generation {generation}:\")\n",
    "        for row in self.board:\n",
    "            print(' ' + ' | '.join(row))\n",
    "            print('---+---+---')\n",
    "        print()\n",
    "\n",
    "    def count_two_in_a_rows(self, player):\n",
    "        # Count the number of two-in-a-rows for a given player\n",
    "        opponent = 'O' if player == 'X' else 'X'\n",
    "        two_in_a_rows = 0\n",
    "        # Check rows, columns, and diagonals for two-in-a-rows\n",
    "        for i in range(3):\n",
    "            if self.board[i].count(player) == 2 and self.board[i].count(opponent) == 0:\n",
    "                two_in_a_rows += 1\n",
    "            column = [self.board[j][i] for j in range(3)]\n",
    "            if column.count(player) == 2 and column.count(opponent) == 0:\n",
    "                two_in_a_rows += 1\n",
    "        diagonals = [[self.board[i][i] for i in range(3)], [self.board[i][2-i] for i in range(3)]]\n",
    "        for diag in diagonals:\n",
    "            if diag.count(player) == 2 and diag.count(opponent) == 0:\n",
    "                two_in_a_rows += 1\n",
    "        return two_in_a_rows\n",
    "\n",
    "    def evaluate_state(self, player):\n",
    "        # Evaluate the board state for a given player\n",
    "        if self.winner == player:\n",
    "            return 100\n",
    "        elif self.winner is not None:\n",
    "            return -100\n",
    "        else:\n",
    "            score = self.count_two_in_a_rows(player) * 10\n",
    "            # Adjust score based on opponent's two-in-a-rows\n",
    "            opponent = 'O' if player == 'X' else 'X'\n",
    "            opponent_two_in_a_rows = self.count_two_in_a_rows(opponent)\n",
    "            if opponent_two_in_a_rows > 0:\n",
    "                score -= opponent_two_in_a_rows * 50\n",
    "            return score\n",
    "\n",
    "    def check_immediate_threat(self, player):\n",
    "        # Check if there's an immediate win available for the opponent\n",
    "        opponent = 'O' if player == 'X' else 'X'\n",
    "        for row in range(3):\n",
    "            for col in range(3):\n",
    "                if self.board[row][col] == ' ':\n",
    "                    temp_board = self.clone()\n",
    "                    temp_board.board[row][col] = opponent\n",
    "                    temp_board.check_winner()\n",
    "                    if temp_board.winner == opponent:\n",
    "                        return True\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "676221a6a09b5c12",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Monte Carlo Search Node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4dce34cf5a0eaa1f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-26T18:06:16.159157100Z",
     "start_time": "2024-02-26T18:06:16.139659400Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class MCTSNode:\n",
    "    def __init__(self, game_state, parent=None, move=None):\n",
    "        self.game_state = game_state\n",
    "        self.parent = parent\n",
    "        self.move = move\n",
    "        self.children = []\n",
    "        self.wins = 0\n",
    "        self.visits = 0\n",
    "        self.untried_moves = game_state.get_legal_moves()\n",
    "\n",
    "    def UCB1(self, exploration_parameter):\n",
    "        # Calculate the Upper Confidence Bound for tree node selection\n",
    "        if self.visits == 0:\n",
    "            return float('inf')\n",
    "        # Balances exploration and exploitation.\n",
    "        return self.wins / self.visits + exploration_parameter * math.sqrt(2 * math.log(self.parent.visits) / self.visits)\n",
    "\n",
    "    def select_child(self, exploration_parameter=2):\n",
    "        # Select a child node using the UCB1 formula\n",
    "        return max(self.children, key=lambda child: child.UCB1(exploration_parameter))\n",
    "\n",
    "    def add_child(self, move, game_state):\n",
    "        # Create a new child node for a given move and game state.\n",
    "        child = MCTSNode(game_state=game_state.clone(), parent=self, move=move)\n",
    "        self.untried_moves.remove(move)\n",
    "        self.children.append(child)\n",
    "        return child\n",
    "\n",
    "    def update(self, result):\n",
    "        # Update this node's win/visit statistics based on simulation result.\n",
    "        self.visits += 1\n",
    "        if result == self.game_state.current_turn:\n",
    "            self.wins += 1  # Win for the current player.\n",
    "        elif result == 'Draw':\n",
    "            self.wins += 0.5  # Half-win for a draw."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "30d087e8a3b038ed",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-26T18:06:16.209473900Z",
     "start_time": "2024-02-26T18:06:16.169696600Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def evaluate_board_for_rollout(board, player):\n",
    "    # Initial variables\n",
    "    opponent = 'O' if player == 'X' else 'X'\n",
    "    center = (1, 1)\n",
    "    corners = [(0, 0), (0, 2), (2, 0), (2, 2)]\n",
    "    edges = [(0, 1), (1, 0), (1, 2), (2, 1)]\n",
    "\n",
    "    # Check for immediate wins or blocks\n",
    "    for move in board.get_legal_moves():\n",
    "        temp_board = board.clone()\n",
    "        temp_board.move(*move)\n",
    "        if temp_board.winner:\n",
    "            return move, True  # Immediate win or block\n",
    "\n",
    "    # Take the center if available\n",
    "    if board.board[center[0]][center[1]] == ' ':\n",
    "        return center, True\n",
    "\n",
    "    # Take a corner, prioritizing those adjacent to opponent's marks to disrupt their strategy\n",
    "    available_corners = [corner for corner in corners if board.board[corner[0]][corner[1]] == ' ']\n",
    "    if available_corners:\n",
    "        return random.choice(available_corners), True\n",
    "\n",
    "    # Take an edge as a last resort\n",
    "    available_edges = [edge for edge in edges if board.board[edge[0]][edge[1]] == ' ']\n",
    "    if available_edges:\n",
    "        return random.choice(available_edges), True\n",
    "\n",
    "    return None, False\n",
    "\n",
    "def rollout_policy(board, player):\n",
    "    # Use a rollout policy to select moves\n",
    "    move, found = evaluate_board_for_rollout(board, player)\n",
    "    if found:\n",
    "        return move"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3ce5e90ff02c5a1",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Monte Carlo Tree Search Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1d0b8dc40b8219b9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-26T18:06:16.209473900Z",
     "start_time": "2024-02-26T18:06:16.189430800Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def MCTS(root_state, iterations, exploration_parameter=2):\n",
    "    root_node = MCTSNode(game_state=root_state)\n",
    "\n",
    "    for _ in range(iterations):\n",
    "        node = root_node\n",
    "        state = root_state.clone()\n",
    "\n",
    "        # Selection with tweaked exploration parameter\n",
    "        while node.untried_moves == [] and node.children != []:\n",
    "            node = node.select_child(exploration_parameter)\n",
    "\n",
    "        # Expansion\n",
    "        if node.untried_moves:\n",
    "            move = random.choice(node.untried_moves)\n",
    "            state.move(*move)\n",
    "            node = node.add_child(move, state)\n",
    "\n",
    "        # Simulation with improved rollout policy\n",
    "        while state.get_legal_moves():\n",
    "            move = rollout_policy(state, state.current_turn)\n",
    "            state.move(*move)\n",
    "\n",
    "        # Backpropagation with nuanced scoring\n",
    "        while node is not None:\n",
    "            node.update(state.evaluate_state(node.game_state.current_turn))\n",
    "            node = node.parent\n",
    "\n",
    "    return sorted(root_node.children, key=lambda c: c.visits)[-1].move"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "25b40f4f160ec772",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-26T18:06:32.798940400Z",
     "start_time": "2024-02-26T18:06:31.904899200Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI makes move at (1, 1):\n",
      "Board State at Generation 1:\n",
      "   |   |  \n",
      "---+---+---\n",
      "   | X |  \n",
      "---+---+---\n",
      "   |   |  \n",
      "---+---+---\n",
      "\n",
      "Opponent makes move at (0, 1):\n",
      "Board State at Generation 2:\n",
      "   | O |  \n",
      "---+---+---\n",
      "   | X |  \n",
      "---+---+---\n",
      "   |   |  \n",
      "---+---+---\n",
      "\n",
      "AI makes move at (0, 2):\n",
      "Board State at Generation 3:\n",
      "   | O | X\n",
      "---+---+---\n",
      "   | X |  \n",
      "---+---+---\n",
      "   |   |  \n",
      "---+---+---\n",
      "\n",
      "Opponent makes move at (0, 0):\n",
      "Board State at Generation 4:\n",
      " O | O | X\n",
      "---+---+---\n",
      "   | X |  \n",
      "---+---+---\n",
      "   |   |  \n",
      "---+---+---\n",
      "\n",
      "AI makes move at (2, 0):\n",
      "Board State at Generation 5:\n",
      " O | O | X\n",
      "---+---+---\n",
      "   | X |  \n",
      "---+---+---\n",
      " X |   |  \n",
      "---+---+---\n",
      "\n",
      "Game over. Winner: AI\n"
     ]
    }
   ],
   "source": [
    "def play_game():\n",
    "    game = TicTacToe()\n",
    "    generation = 0\n",
    "    \n",
    "    # Play the game until it's over\n",
    "    while not game.game_over:\n",
    "        # AI's turn\n",
    "        if game.current_turn == 'X':\n",
    "            move = MCTS(game.clone(), iterations=1000)\n",
    "            print(f\"AI makes move at {move}:\")\n",
    "            game.move(*move)\n",
    "            generation += 1\n",
    "            game.print_board(generation)\n",
    "        # Opponent's turn\n",
    "        else:\n",
    "            possible_moves = game.get_legal_moves()\n",
    "            if possible_moves:\n",
    "                move = random.choice(possible_moves)\n",
    "                print(f\"Opponent makes move at {move}:\")\n",
    "                game.move(*move)\n",
    "                generation += 1\n",
    "                game.print_board(generation)\n",
    "            else:\n",
    "                print(\"No legal moves available. Game over.\")\n",
    "                break\n",
    "\n",
    "    # Announce the game result.\n",
    "    if game.winner:\n",
    "        print(f\"Game over. Winner: {'AI' if game.winner == 'X' else 'Opponent'}\")\n",
    "    else:\n",
    "        print(\"Game over. It's a draw.\")\n",
    "\n",
    "play_game()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed576586",
   "metadata": {},
   "source": [
    "- **Roll-Out Policy:**    \n",
    "For the roll-out policy, which determines the moves during the simulation phase of MCTS, we decided for a simplified heuristic approach. This approach prioritizes moves that can lead to immediate wins or blocks, captures the center if available, select for corners to potentially create multiple winning paths, and finally selects edges. This hierarchy mirrors common Tic-Tac-Toe strategies where control of the center and corners can lead to multiple winning paths and block opponent's opportunities. This approach is fast and ensures that the simulation phase doesn't randomly move through all possible game states, which improves both speed and relevance of the simulations. Additionally, it reduces the variance in the simulation outcomes, making the simulations more meaningful and the AI more competitive, as it simulates somewhat rational play instead of random moves.     \n",
    "\n",
    "- **Opponent Policy:**    \n",
    "The opponent’s policy is implicitly defined through the game’s mechanics and the MCTS process itself. During the simulation phase, moves are made for both the AI and the opponent based on the rollout policy, which aims to be as rational as possible within the constraints of Tic-Tac-Toe. By simulating the opponent’s moves using the same heuristic criteria, the AI prepares against a competent player, but it might not necessarily optimize for the most challenging adversary. The is reason is that this assumption simplifies the model but does not account for potentially irrational or unpredictable human play. In real-world scenarios, opponents may not always make the optimal move, and thus, while this policy strengthens the AI against rational moves, it might not fully prepare it for all possible human strategies.\n",
    "\n",
    "- **Selection Policy:**     \n",
    "The selection policy utilizes the Upper Confidence Bound (UCB1) formula, which balances the exploration of less-visited nodes with the exploitation of nodes with a high win ratio. This balance is crucial because it prevents the algorithm from over-focusing on early successful paths (exploitation) and encourages the discovery of potentially better paths that have not been explored thoroughly (exploration). The UCB1 formula's efficiency lies in its mathematical foundation, which statistically ensures that given enough time, the algorithm will explore all parts of the tree adequately. This method is particularly well-suited for the finite and discrete nature of Tic-Tac-Toe's game space.\n",
    "\n",
    "- **Updates to the Search Tree (Back-up):**\n",
    "The back-up step is where the results of the simulations are propagated back up the tree to update the statistics of the nodes. This process is implemented through the `update method` in the `MCTSNode class`. This process ensures that the nodes reflect the latest information about the likelihood of winning from that position. The update method adjusts the wins and visits based on the outcome (win, draw, or loss), which influences future selections and roll-outs. This feedback loop, facilitated by a scoring mechanism, is what allows the AI to learn and adapt its strategy over time. This approach is particularly effective in games with clear win/loss outcomes at each node. However, the binary nature of the updates (win/loss with a simplistic scoring mechanism) might oversimplify complex strategic considerations in games more complex than Tic-Tac-Toe.   \n",
    "\n",
    "- **Sampling in MC:**     \n",
    "    1. Selection Phase: During the selection phase, the algorithm uses the Upper Confidence Bound 1 (UCB1) formula to select nodes for exploration. This is not random sampling per se but a strategic selection process that balances exploration and exploitation based on the outcomes of previous simulations. The exploration parameter in the UCB1 formula influences the degree of randomness in selection: a higher value increases the emphasis on exploration (thus introducing more variability or \"sampling\" of less-visited nodes), whereas a lower value emphasizes exploitation of known good paths.   \n",
    "\n",
    "    2. Expansion Phase: When a new move is explored, and a node is added to the tree, this expansion is based on the available legal moves from the current game state. The choice of which move to explore next could be considered a form of sampling, especially when the selection is made randomly from the set of untried moves, as is the case here.   \n",
    "\n",
    "    3. Simulation Phase (Rollout): This is where the core of the sampling occurs. In the simulation phase, the game is played out to the end (a win, loss, or draw) from the current state using a predefined policy. The rollout_policy function dictates this process, but instead of purely random moves, it uses a heuristic-based strategy that mimics rational play: checking for immediate wins or blocks, prioritizing center, corners, and then edges. This method still represents a form of sampling but with a bias towards strategic moves. It's a deviation from traditional Monte Carlo sampling, which would involve completely random moves. The choice to use a heuristic approach is a design decision to increase the efficiency and relevance of the simulations by reflecting more realistic game scenarios.  \n",
    "\n",
    "    4. Backpropagation Phase: After each simulation, the results (win, loss, draw) are propagated back up the tree, updating the statistics (wins, visits) of the nodes along the path taken. This step doesn't involve sampling but affects future sampling decisions by altering the UCB1 scores based on new information, thus guiding where the algorithm samples next.   \n",
    "\n",
    "- **Evaluation of the Algorithm:**     \n",
    "    - **Pros**:    \n",
    "        - Adaptability: MCTS does not require a comprehensive understanding of the game's strategy, making it versatile for different games, including Tic-Tac-Toe.    \n",
    "        - Competitiveness: The algorithm can play at a competitive level, especially with a well-implemented roll-out policy and sufficient iterations for exploration and exploitation. For casual players or those without a clear strategy, the algorithm could prove to be challenging due ot its strategic roll-out policy and ability to learn and adap over the course of the game.\n",
    "        - No Need for a Predefined Strategy: It discovers optimal moves through exploration and exploitation, reducing the need for hardcoded strategies.\n",
    "        - Fast and Efficient: For a relatively simple game like Tic-Tac-Toe, this MCTS implementation can be considered fast, especially since the decision space is limited. However, the speed could decrease as the number of iterations increases to ensure more accurate and competitive gameplay. For real-time decision-making in Tic-Tac-Toe, the algorithm is sufficiently fast, but for more complex games or larger grids, performance optimizations might be necessary.\n",
    "        \n",
    "\n",
    "    - **Cons**:   \n",
    "        - Computational Demands: The need for numerous simulations for each move decision can be computationally expensive, especially as the game progresses and possibilities decrease.     \n",
    "        - Sample Efficiency: MCTS may require many iterations to converge on an effective strategy, particularly against diverse or unpredictable opponents, which is not sample efficient.     \n",
    "        - Scalability: Scaling MCTS to larger grids (e.g., 4x4, 5x5) increases the computational complexity exponentially, making it less practical for more extensive games without optimization.     \n",
    "        - Predictability: The algorithm's strategy might become predictable, especially if the roll-out policy is not sufficiently complex or if the exploration parameter is not well-tuned. This means that it can lose. A human player could beat or draw against this algorithm by using a good strategy. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88753424",
   "metadata": {},
   "source": [
    "## References\n",
    "***\n",
    "\n",
    "- Choudhary, A. (2018). Reinforcement Learning Guide: Solving the Multi-Armed Bandit Problem from Scratch in Python. [online] Analytics Vidhya. Available at: https://www.analyticsvidhya.com/blog/2018/09/reinforcement-multi-armed-bandit-scratch-python/ [Accessed 23 Feb. 2024]."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c902e53e01be6b5",
   "metadata": {
    "id": "3c902e53e01be6b5"
   },
   "source": [
    "## Self Check\n",
    "***\n",
    "- Have you answered all questions to the best of your ability?\n",
    "Yes, we have.\n",
    "- Is all the required information on the front page, is the file name correct etc.?\n",
    "Indeed, all the required information on the front page has been included.\n",
    "- Anything else you can easily check? (details, terminology, arguments, clearly stated answers etc.?)\n",
    "We have checked, and everything looks good."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
